import pandas as pd
import numpy as np
import torch
from torch_geometric.data import Data, Dataset
# ä¼˜å…ˆä½¿ç”¨PyGä¸“ç”¨DataLoaderï¼Œè‡ªåŠ¨å¤„ç†å›¾æ‰¹æ¬¡æ‹¼æ¥ï¼Œé¿å…tupleæŠ¥é”™
from torch_geometric.loader import DataLoader as PyGDataLoader
# å•ç‹¬å¯¼å…¥å®Œæ•´ç‰ˆ get_laplacianï¼ˆæ”¯æŒ normalized å‚æ•°ï¼‰
from torch_geometric.utils.laplacian import get_laplacian
# å…¶ä»–å·¥å…·å‡½æ•°ä¿æŒåŸæœ‰å¯¼å…¥
from torch_geometric.utils import to_scipy_sparse_matrix
from scipy.sparse.linalg import eigs
import os
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import warnings
import random
from typing import List, Tuple, Dict, Optional
warnings.filterwarnings('ignore')


class GraphPatchDataset(Dataset):
    """å›¾å—æ•°æ®é›†ç±»ï¼Œç»§æ‰¿è‡ªPyG Dataset"""
    
    def __init__(self, patches: List[Data], patch_gdps: List[float], 
                 transform=None, pre_transform=None):
        """
        åˆå§‹åŒ–å›¾å—æ•°æ®é›†
        
        Args:
            patches: å›¾å—æ•°æ®åˆ—è¡¨
            patch_gdps: å›¾å—GDPå€¼åˆ—è¡¨ï¼ˆåŸå§‹å€¼ï¼‰
            transform: æ•°æ®å˜æ¢
            pre_transform: é¢„å˜æ¢
        """
        super().__init__(transform=transform, pre_transform=pre_transform)
        self.patches = patches
        self.patch_gdps = patch_gdps  # ä¿å­˜åŸå§‹GDPå€¼
        
        # ä¸ºæ¯ä¸ªå›¾å—è®¾ç½®ä¸¤ä¸ªæ ‡ç­¾ï¼šåŸå§‹GDPå’Œlog(1+GDP)ï¼ˆå›¾å—çº§å…¨å±€æ ‡ç­¾ï¼‰
        # åŒæ—¶ä¿ç•™èŠ‚ç‚¹çº§æ ‡ç­¾ï¼ˆpatch.y_node å·²åœ¨æ„å»ºæ—¶å­˜å…¥ï¼‰
        for patch, gdp in zip(self.patches, self.patch_gdps):
            # è®¡ç®—log(1+GDP)
            log_gdp = np.log1p(gdp)
            
            # è®¾ç½®å›¾å—çº§å…¨å±€æ ‡ç­¾ï¼šåŸå§‹å€¼å’Œlogå€¼ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            patch.y = torch.tensor([[gdp, log_gdp]], dtype=torch.float)
        
        # åˆ›å»ºç´¢å¼•æ˜ å°„
        self._indices = list(range(len(self.patches)))
    
    def len(self):
        """PyG Datasetè¦æ±‚çš„lenæ–¹æ³•"""
        return len(self.patches)
    
    def __len__(self):
        """Pythonæ ‡å‡†çš„__len__æ–¹æ³•"""
        return len(self.patches)
    
    def get(self, idx):
        """PyG Datasetè¦æ±‚çš„getæ–¹æ³•ï¼ˆæ ¸å¿ƒï¼šè¿”å›å•ä¸ªPyG Dataå¯¹è±¡ï¼Œé¿å…tupleï¼‰"""
        return self.patches[idx]
    
    def __getitem__(self, idx):
        """æ”¯æŒç´¢å¼•è®¿é—®ï¼ˆè¿”å›å•ä¸ªPyG Dataå¯¹è±¡ï¼Œå…¼å®¹DataLoaderï¼‰"""
        if isinstance(idx, slice):
            return GraphPatchDataset(self.patches[idx], self.patch_gdps[idx])
        return self.patches[idx]
    
    def indices(self):
        """PyG Datasetè¦æ±‚çš„indicesæ–¹æ³•"""
        return self._indices
    
    def split_by_county(self, county_names: List[str], test_size: float = 0.2, 
                       random_state: int = 42):
        """
        æŒ‰å¿åˆ’åˆ†æ•°æ®é›†ï¼ˆé˜²æ­¢æ•°æ®æ³„éœ²ï¼‰
        
        Args:
            county_names: æ¯ä¸ªå›¾å—æ‰€å±çš„å¿ååˆ—è¡¨ï¼ˆé•¿åº¦éœ€ä¸patchesç›¸åŒï¼‰
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
            
        Returns:
            train_dataset, val_dataset, test_dataset: åˆ’åˆ†åçš„æ•°æ®é›†
        """
        # æ ¡éªŒè¾“å…¥é•¿åº¦
        if len(county_names) != len(self.patches):
            raise ValueError("å¿ååˆ—è¡¨é•¿åº¦å¿…é¡»ä¸å›¾å—åˆ—è¡¨é•¿åº¦ä¸€è‡´")
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„å¿
        unique_counties = list(set(county_names))
        
        # åˆ’åˆ†è®­ç»ƒå¿å’Œæµ‹è¯•å¿
        train_counties, test_counties = train_test_split(
            unique_counties, test_size=test_size, random_state=random_state
        )
        
        # ä»æµ‹è¯•å¿ä¸­å†åˆ’åˆ†éªŒè¯å¿
        test_counties, val_counties = train_test_split(
            test_counties, test_size=0.5, random_state=random_state
        )
        
        # æ ¹æ®å¿åˆ’åˆ†æ•°æ®é›†
        train_indices = [i for i, county in enumerate(county_names) 
                        if county in train_counties]
        val_indices = [i for i, county in enumerate(county_names) 
                      if county in val_counties]
        test_indices = [i for i, county in enumerate(county_names) 
                       if county in test_counties]
        
        # åˆ›å»ºå­æ•°æ®é›†
        train_dataset = self.subset(train_indices)
        val_dataset = self.subset(val_indices)
        test_dataset = self.subset(test_indices)
        
        print(f"ğŸ“Š æŒ‰å¿åˆ’åˆ†æ•°æ®é›†:")
        print(f"  è®­ç»ƒå¿: {len(train_counties)} ä¸ª, å›¾å—: {len(train_indices)} ä¸ª")
        print(f"  éªŒè¯å¿: {len(val_counties)} ä¸ª, å›¾å—: {len(val_indices)} ä¸ª")
        print(f"  æµ‹è¯•å¿: {len(test_counties)} ä¸ª, å›¾å—: {len(test_indices)} ä¸ª")
        
        return train_dataset, val_dataset, test_dataset
    
    def split_random(self, train_ratio: float = 0.7, val_ratio: float = 0.15, 
                    test_ratio: float = 0.15, random_state: int = 42):
        """
        éšæœºåˆ’åˆ†æ•°æ®é›†
        
        Args:
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
            
        Returns:
            train_dataset, val_dataset, test_dataset: åˆ’åˆ†åçš„æ•°æ®é›†
        """
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"
        
        # è®¾ç½®éšæœºç§å­
        if random_state is not None:
            random.seed(random_state)
            np.random.seed(random_state)
        
        # æ‰“ä¹±ç´¢å¼•
        indices = self._indices.copy()
        random.shuffle(indices)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹
        n_total = len(indices)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        
        # åˆ’åˆ†ç´¢å¼•
        train_indices = indices[:n_train]
        val_indices = indices[n_train:n_train + n_val]
        test_indices = indices[n_train + n_val:]
        
        # åˆ›å»ºå­æ•°æ®é›†
        train_dataset = self.subset(train_indices)
        val_dataset = self.subset(val_indices)
        test_dataset = self.subset(test_indices)
        
        print(f"ğŸ“Š éšæœºåˆ’åˆ†æ•°æ®é›†:")
        print(f"  è®­ç»ƒé›†: {len(train_indices)} ä¸ªå›¾å— ({train_ratio*100:.1f}%)")
        print(f"  éªŒè¯é›†: {len(val_indices)} ä¸ªå›¾å— ({val_ratio*100:.1f}%)")
        print(f"  æµ‹è¯•é›†: {len(test_indices)} ä¸ªå›¾å— ({test_ratio*100:.1f}%)")
        
        return train_dataset, val_dataset, test_dataset
    
    def subset(self, indices: List[int]):
        """åˆ›å»ºå­é›†ï¼ˆè¿”å›GraphPatchDatasetå®ä¾‹ï¼Œä¿æŒæ•°æ®ç»“æ„ä¸€è‡´ï¼‰"""
        if not indices:
            return GraphPatchDataset([], [])
        
        subset_patches = [self.patches[i] for i in indices]
        subset_gdps = [self.patch_gdps[i] for i in indices]
        
        return GraphPatchDataset(subset_patches, subset_gdps)
    
    def get_statistics(self):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–°å¢èŠ‚ç‚¹çº§æ ‡ç­¾ç»Ÿè®¡ï¼‰"""
        if len(self.patches) == 0:
            return {}
        
        gdps = np.array(self.patch_gdps)
        log_gdps = np.log1p(gdps)  # è®¡ç®—log(1+GDP)
        num_nodes = [patch.num_nodes for patch in self.patches]
        num_edges = [patch.edge_index.shape[1] for patch in self.patches]
        
        # æ–°å¢ï¼šèŠ‚ç‚¹çº§æ ‡ç­¾ç»Ÿè®¡
        node_gdp_list = []
        node_log_gdp_list = []
        for patch in self.patches:
            if hasattr(patch, 'y_node') and patch.y_node is not None:
                # æå–è¯¥å›¾å—æ‰€æœ‰èŠ‚ç‚¹çš„GDPå’ŒlogGDP
                node_gdps = patch.y_node[:, 0].numpy()
                node_log_gdps = patch.y_node[:, 1].numpy()
                node_gdp_list.extend(node_gdps)
                node_log_gdp_list.extend(node_log_gdps)
        
        # ä¿®å¤åŸä»£ç ä¸­çš„typoï¼ˆmax_log_gdpè¯¯å†™ä¸ºmax_node_log_gdpï¼‰
        min_log_gdp = float(np.min(log_gdps))
        max_log_gdp = float(np.max(log_gdps))
        
        # å°†æ‰€æœ‰numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
        stats = {
            'num_patches': int(len(self.patches)),
            'avg_nodes': float(np.mean(num_nodes)),
            'std_nodes': float(np.std(num_nodes)),
            'min_nodes': int(np.min(num_nodes)),
            'max_nodes': int(np.max(num_nodes)),
            'avg_edges': float(np.mean(num_edges)),
            'avg_gdp': float(np.mean(gdps)),
            'std_gdp': float(np.std(gdps)),
            'min_gdp': float(np.min(gdps)),
            'max_gdp': float(np.max(gdps)),
            'avg_log_gdp': float(np.mean(log_gdps)),
            'std_log_gdp': float(np.std(log_gdps)),
            'min_log_gdp': min_log_gdp,
            'max_log_gdp': max_log_gdp,
            'feature_dim': int(self.patches[0].x.shape[1] if self.patches else 0),
            'has_lappe': bool(hasattr(self.patches[0], 'lap_pe') and self.patches[0].lap_pe is not None) if self.patches else False,
            # æ–°å¢ï¼šèŠ‚ç‚¹çº§æ ‡ç­¾ç»Ÿè®¡
            'has_node_labels': bool(len(node_gdp_list) > 0),
            'avg_node_gdp': float(np.mean(node_gdp_list)) if node_gdp_list else 0.0,
            'std_node_gdp': float(np.std(node_gdp_list)) if node_gdp_list else 0.0,
            'min_node_gdp': float(np.min(node_gdp_list)) if node_gdp_list else 0.0,
            'max_node_gdp': float(np.max(node_gdp_list)) if node_gdp_list else 0.0,
            'avg_node_log_gdp': float(np.mean(node_log_gdp_list)) if node_log_gdp_list else 0.0,
            'std_node_log_gdp': float(np.std(node_log_gdp_list)) if node_log_gdp_list else 0.0,
        }
        
        return stats


class GraphDataBuilder:
    """å›¾æ•°æ®æ„å»ºå™¨ï¼šæ•´åˆå›¾æ„å»ºã€å›¾å—ç”Ÿæˆå’Œä¼ªæ ‡ç­¾ç”Ÿæˆï¼ˆæ–°å¢èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰"""
    
    def __init__(self, gdp_file_path, patch_size=16, lap_pe_k=10):
        """
        åˆå§‹åŒ–å›¾æ•°æ®æ„å»ºå™¨
        
        Args:
            gdp_file_path: GDPæ•°æ®æ–‡ä»¶è·¯å¾„
            patch_size: å›¾å—å°ºå¯¸ï¼ˆé»˜è®¤16Ã—16ï¼‰
            lap_pe_k: LapPEç¼–ç çš„ç‰¹å¾ç»´åº¦ï¼ˆå–å‰kä¸ªæœ€å°éé›¶ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼‰
        """
        self.gdp_file_path = gdp_file_path
        self.patch_size = patch_size
        self.lap_pe_k = lap_pe_k  # LapPE ç‰¹å¾ç»´åº¦
        
        # ä¿®å¤åŸä»£ç typoï¼šis_fittedï¼ˆåŸä»£ç ä¸ºis_fittedï¼‰
        self.is_fitted = False
        self.feature_columns_to_scale = None
        self.all_feature_columns = None
        self.cnn_feature_columns = None
        
        # å­˜å‚¨æ•°æ®
        self.all_dataframes = {}
        self.patch_county_mapping = []  # åˆå§‹åŒ–å¿æ˜ å°„ï¼ˆä¿®å¤åŸæœ‰æœªå®šä¹‰é—®é¢˜ï¼‰
        self.scaler = StandardScaler()  # æ˜¾å¼åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æŠ¥é”™
    
    # ==================== LapPE ç¼–ç éƒ¨åˆ†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼Œæ— ä¿®æ”¹ï¼‰ ====================
    def compute_laplacian_positional_encoding(self, graph_data: Data) -> torch.Tensor:
        """
        è®¡ç®—å›¾çš„æ‹‰æ™®æ‹‰æ–¯ä½ç½®ç¼–ç ï¼ˆLapPEï¼‰- æ­£ç¡®ä½¿ç”¨ normalization å‚æ•°
        """
        num_nodes = graph_data.num_nodes
        
        # å¤„ç†ç©ºå›¾æˆ–å•èŠ‚ç‚¹å›¾ï¼ˆæ— æ³•è®¡ç®—æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼‰
        if num_nodes <= 1 or graph_data.edge_index.shape[1] == 0:
            return torch.zeros((num_nodes, self.lap_pe_k), dtype=torch.float)
        
        # æ­¥éª¤1ï¼šè®¡ç®—å¯¹ç§°å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼ˆæ­£ç¡®ä½¿ç”¨ normalization="sym"ï¼‰
        laplacian_edge_index, laplacian_edge_weight = get_laplacian(
            edge_index=graph_data.edge_index,
            normalization="sym",  # æ›¿æ¢ normalized=True ä¸º normalization="sym"ï¼Œå®ç°å¯¹ç§°å½’ä¸€åŒ–
            num_nodes=num_nodes   # ä¼ å…¥èŠ‚ç‚¹æ•°æé«˜é²æ£’æ€§
        )
        
        # è½¬æ¢ä¸ºScipyç¨€ç–çŸ©é˜µæ ¼å¼ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šä½ç½®å‚æ•°ä¼ å…¥è¾¹æƒé‡ï¼‰
        laplacian_sparse = to_scipy_sparse_matrix(
            laplacian_edge_index,  # ç¬¬1ä¸ªä½ç½®å‚æ•°ï¼šè¾¹ç´¢å¼•ï¼ˆå¿…ä¼ ï¼‰
            laplacian_edge_weight, # ç¬¬2ä¸ªä½ç½®å‚æ•°ï¼šè¾¹æƒé‡ï¼ˆå»æ‰ edge_weight= å…³é”®å­—ï¼‰
            num_nodes=num_nodes    # ç¬¬3ä¸ªå‚æ•°ï¼šèŠ‚ç‚¹æ•°ï¼ˆåˆæ³•å…³é”®å­—å‚æ•°ï¼Œä¿ç•™ï¼‰
        )
        
        try:
            eigvals, eigvecs = eigs(
                laplacian_sparse,
                k=self.lap_pe_k + 1,
                which='SM',
                return_eigenvectors=True
            )
            
            eigvals_real = np.real(eigvals)
            eigvecs_real = np.real(eigvecs)
            
            non_zero_mask = eigvals_real > 1e-8
            non_zero_eigvals = eigvals_real[non_zero_mask]
            non_zero_eigvecs = eigvecs_real[:, non_zero_mask]
            
            if len(non_zero_eigvals) >= self.lap_pe_k:
                sorted_indices = np.argsort(non_zero_eigvals)[:self.lap_pe_k]
                lap_pe = non_zero_eigvecs[:, sorted_indices]
            else:
                lap_pe = np.zeros((num_nodes, self.lap_pe_k), dtype=np.float64)
                lap_pe[:, :len(non_zero_eigvals)] = non_zero_eigvecs[:, :len(non_zero_eigvals)]
            
            return torch.tensor(lap_pe, dtype=torch.float)
        
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—LapPEç¼–ç å¤±è´¥: {e}ï¼Œè¿”å›é›¶çŸ©é˜µ")
            return torch.zeros((num_nodes, self.lap_pe_k), dtype=torch.float)
    
    def merge_lap_pe_with_node_features(self, graph_data: Data) -> Data:
        """
        å°†LapPEç¼–ç ä¸åŸå§‹èŠ‚ç‚¹ç‰¹å¾åˆå¹¶ï¼ˆå¯é€‰ï¼šæ›¿æ¢/æ‹¼æ¥ï¼‰
        æ­¤å¤„é‡‡ç”¨æ‹¼æ¥æ–¹å¼ï¼Œä¿æŒåŸå§‹ç‰¹å¾ä¸å˜ï¼Œæ–°å¢ä½ç½®ç¼–ç ä¿¡æ¯
        """
        # è®¡ç®—LapPEç¼–ç 
        lap_pe = self.compute_laplacian_positional_encoding(graph_data)
        
        # å­˜å‚¨LapPEç¼–ç ä½œä¸ºå›¾çš„ç‹¬ç«‹å±æ€§ï¼ˆæ¨èï¼Œä¸ç ´ååŸå§‹ç‰¹å¾ï¼‰
        graph_data.lap_pe = lap_pe
        
        return graph_data
    
    # ==================== GDPæ•°æ®åŠ è½½éƒ¨åˆ†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼Œæ— ä¿®æ”¹ï¼‰ ====================
    def load_county_gdp_dict(self):
        """åŠ è½½å¿GDPæ•°æ®å­—å…¸"""
        try:
            gdp_df = pd.read_excel(self.gdp_file_path)
            
            gdp_dict = {}
            for _, row in gdp_df.iterrows():
                region_code = row.iloc[0]
                gdp_2020 = row.iloc[1]
                
                if pd.notna(region_code) and pd.notna(gdp_2020) and gdp_2020 > 0:
                    try:
                        code_int = int(region_code)
                        gdp_dict[code_int] = float(gdp_2020)
                    except (ValueError, TypeError):
                        continue
            
            print(f"âœ… GDPæ•°æ®åŠ è½½å®Œæˆ: {len(gdp_dict)} ä¸ªå¿")
            return gdp_dict
            
        except Exception as e:
            print(f"âŒ è¯»å–GDPæ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def match_county_names(self, feature_files, gdp_dict):
        """åŒ¹é…ç‰¹å¾æ–‡ä»¶åå’Œè¡Œæ”¿åŒºåŸŸä»£ç ï¼Œè¾“å‡ºæœªåŒ¹é…æˆåŠŸçš„å¿å"""
        matched_data = {}
        unmatched_counties = []  # æ–°å¢ï¼šæ”¶é›†æœªåŒ¹é…æˆåŠŸçš„å¿å
        
        for feature_file in feature_files:
            county_name = feature_file.replace('_features.csv', '')
            
            try:
                code_int = int(county_name)
                if code_int in gdp_dict:
                    matched_data[county_name] = gdp_dict[code_int]
                else:
                    # æ–°å¢ï¼šå¿åå¯è½¬ä¸ºæ•°å­—ï¼Œä½†ä¸åœ¨gdp_dictä¸­ï¼ˆæœªåŒ¹é…ï¼‰
                    unmatched_counties.append(county_name)
            except ValueError:
                # æ–°å¢ï¼šå¿åæ— æ³•è½¬ä¸ºæ•°å­—ï¼ˆæ ¼å¼é”™è¯¯ï¼ŒæœªåŒ¹é…ï¼‰
                unmatched_counties.append(county_name)
                continue
        
        # åŸæœ‰ï¼šæ‰“å°åŒ¹é…æˆåŠŸä¿¡æ¯
        print(f"âœ… å¿ååŒ¹é…å®Œæˆ: æˆåŠŸ {len(matched_data)} ä¸ªå¿")
        
        # æ–°å¢ï¼šæ‰“å°æœªåŒ¹é…ä¿¡æ¯ï¼ˆåˆ†æƒ…å†µï¼Œæ›´å‹å¥½ï¼‰
        if unmatched_counties:
            # å»é‡ï¼ˆé¿å…é‡å¤æ–‡ä»¶å¯¼è‡´é‡å¤è®°å½•ï¼‰ï¼Œæ’åºï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
            unique_unmatched = sorted(list(set(unmatched_counties)))
            print(f"âŒ æœªåŒ¹é…æˆåŠŸçš„å¿åå…± {len(unique_unmatched)} ä¸ªï¼š")
            # æ ¼å¼åŒ–è¾“å‡ºï¼Œæ¯è¡Œæ˜¾ç¤º5ä¸ªï¼Œé¿å…è¿‡é•¿åˆ·å±
            for i in range(0, len(unique_unmatched), 5):
                batch = unique_unmatched[i:i+5]
                print(f"   {' | '.join(batch)}")
        else:
            print(f"ğŸ‰ æ‰€æœ‰å¿åéƒ½åŒ¹é…æˆåŠŸï¼Œæ— é—æ¼ï¼")
        
        return matched_data
    
    # ==================== ç‰¹å¾å¤„ç†éƒ¨åˆ†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼Œæ— ä¿®æ”¹ï¼‰ ====================
    def _get_feature_columns(self, df):
        # """è·å–æ‰€æœ‰ç‰¹å¾åˆ—ï¼Œå¹¶åŒºåˆ†éœ€è¦æ ‡å‡†åŒ–çš„åˆ—å’ŒCNNç‰¹å¾åˆ—"""
        # exclude_cols = ['county_name', 'position_row', 'position_col', 
        #                'grid_gdp', 'grid_gdp_log', 'weight', 'nl_pop_product']
        # exclude_cols = ['county_name', 'position_row', 'position_col', 
        #            'grid_gdp', 'grid_gdp_log', 'weight', 'nl_pop_product',
        #            'nl_intensity', 'population_density']

        # 1. å®šä¹‰åŸºç¡€æ’é™¤åˆ—ï¼ˆä½ åŸæœ¬æŒ‡å®šçš„ï¼‰
        exclude_cols = [
            'county_name', 'position_row', 'position_col', 
            'grid_gdp', 'grid_gdp_log', 'weight', 'nl_pop_product'
        ]

        # 2. æ‰‹åŠ¨åˆ—å‡ºæ‰€æœ‰POIåˆ—ï¼ˆé€‚é…ä½ ç»™å‡ºçš„åˆ—åï¼Œç¡®ä¿æ— é—æ¼ï¼‰
        poi_cols = [
            'poi_total_count', 'poi_é¤é¥®ç¾é£Ÿ', 'poi_å…¬å¸ä¼ä¸š', 'poi_è´­ç‰©æ¶ˆè´¹',
            'poi_äº¤é€šè®¾æ–½', 'poi_é‡‘èæœºæ„', 'poi_é…’åº—ä½å®¿', 'poi_ç§‘æ•™æ–‡åŒ–',
            'poi_æ—…æ¸¸æ™¯ç‚¹', 'poi_æ±½è½¦ç›¸å…³', 'poi_å•†åŠ¡ä½å®…', 'poi_ç”Ÿæ´»æœåŠ¡',
            'poi_ä¼‘é—²å¨±ä¹', 'poi_åŒ»ç–—ä¿å¥', 'poi_è¿åŠ¨å¥èº«'
        ]

        # 3. æ‰‹åŠ¨åˆ—å‡ºæ‰€æœ‰åœŸåœ°åˆ©ç”¨åˆ—ï¼ˆé€‚é…ä½ ç»™å‡ºçš„åˆ—åï¼Œæ— é—æ¼ï¼‰
        landuse_cols = [
            'landuse_11', 'landuse_12', 'landuse_21', 'landuse_22', 'landuse_23', 
            'landuse_24', 'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41', 
            'landuse_42', 'landuse_43', 'landuse_45', 'landuse_46', 'landuse_51', 
            'landuse_52', 'landuse_53', 'landuse_64', 'landuse_65', 'landuse_66', 
            'landuse_99'
        ]

        # 4. åˆå¹¶æ‰€æœ‰éœ€è¦æ’é™¤çš„åˆ—ï¼ˆåŸºç¡€åˆ—+POIåˆ—+åœŸåœ°åˆ©ç”¨åˆ—ï¼‰ï¼Œå»é‡é¿å…é‡å¤
        exclude_cols = list(set(exclude_cols + poi_cols))

        numeric_cols = [col for col in df.columns 
                       if df[col].dtype in [np.int64, np.float64]]
        
        all_feature_cols = [col for col in numeric_cols 
                           if col not in exclude_cols]
        
        cnn_feature_cols = [col for col in all_feature_cols 
                           if col.startswith('rs_feature_')]
        
        features_to_scale = [col for col in all_feature_cols 
                            if not col.startswith('rs_feature_')]
        
        return all_feature_cols, features_to_scale, cnn_feature_cols
    
    def _calculate_grid_gdp_labels(self, df, county_total_gdp):
        """è®¡ç®—æ¯ä¸ªç½‘æ ¼çš„GDPä¼ªæ ‡ç­¾ï¼šGDP âˆ NL Ã— POPï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        df['weight'] = df['nl_intensity'] * df['population_density']
        total_weight = df['weight'].sum()
        
        if total_weight == 0:
            df['weight'] = 1 / len(df)
            total_weight = 1
        
        # è®¡ç®—åŸå§‹GDP
        df['grid_gdp'] = (df['weight'] / total_weight) * county_total_gdp
        
        # è®¡ç®—log(1+GDP)æ ‡ç­¾
        df['grid_gdp_log'] = np.log1p(df['grid_gdp'])
        
        return df
    
    def load_and_preprocess_county(self, features_dir, county_name, county_gdp):
        """åŠ è½½å¹¶é¢„å¤„ç†å•ä¸ªå¿çš„æ•°æ®"""
        file_path = os.path.join(features_dir, f'{county_name}_features.csv')
        if not os.path.exists(file_path):
            return None
        
        df = pd.read_csv(file_path)
        if len(df) == 0:
            return None
        
        # è®¡ç®—GDPä¼ªæ ‡ç­¾ï¼ˆåŒ…æ‹¬åŸå§‹å€¼å’Œlogå€¼ï¼‰
        df = self._calculate_grid_gdp_labels(df, county_gdp)
        
        # è·å–ç‰¹å¾åˆ—
        if self.all_feature_columns is None:
            self.all_feature_columns, self.feature_columns_to_scale, self.cnn_feature_columns = self._get_feature_columns(df)
        
        # å­˜å‚¨åŸå§‹æ•°æ®
        self.all_dataframes[county_name] = df
        
        return df
    
    def fit_scaler(self, features_dir, county_gdp_dict):
        """æ‹Ÿåˆæ ‡å‡†åŒ–å™¨"""
        print("ğŸ”§ æ‹Ÿåˆç‰¹å¾æ ‡å‡†åŒ–å™¨...")
        
        all_features_to_scale = []
        
        for county_name, county_gdp in tqdm(county_gdp_dict.items(), desc="æ”¶é›†æ ‡å‡†åŒ–æ•°æ®"):
            df = self.load_and_preprocess_county(features_dir, county_name, county_gdp)
            if df is not None and self.feature_columns_to_scale:
                features = df[self.feature_columns_to_scale].values
                all_features_to_scale.append(features)
        
        if all_features_to_scale:
            all_features_to_scale = np.vstack(all_features_to_scale)
            self.scaler.fit(all_features_to_scale)
            self.is_fitted = True
            print(f"âœ… æ ‡å‡†åŒ–å™¨æ‹Ÿåˆå®Œæˆï¼Œå¤„ç† {all_features_to_scale.shape[0]} ä¸ªæ ·æœ¬")
        else:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ç”¨äºæ‹Ÿåˆæ ‡å‡†åŒ–å™¨")
    
    # ==================== å›¾æ„å»ºéƒ¨åˆ†ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šæ–°å¢èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰ ====================
    def build_8_neighbor_edges(self, positions):
        """
        æ„å»º8-é‚»åŸŸè¿æ¥çš„è¾¹ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼Œæ— ä¿®æ”¹ï¼‰
        """
        positions_dict = {pos: idx for idx, pos in enumerate(positions)}
        edges = []
        
        # 8ä¸ªæ–¹å‘
        directions = [
            (0, 1),    # ä¸œ
            (1, 0),    # å—
            (0, -1),   # è¥¿
            (-1, 0),   # åŒ—
            (-1, 1),   # ä¸œåŒ—
            (-1, -1),  # è¥¿åŒ—
            (1, 1),    # ä¸œå—
            (1, -1)    # è¥¿å—ï¼ˆä¹‹å‰é—æ¼ï¼Œè¡¥å…¨8é‚»åŸŸï¼‰
        ]
        
        for pos_idx, (row, col) in enumerate(positions):
            for dr, dc in directions:
                neighbor_pos = (row + dr, col + dc)
                if neighbor_pos in positions_dict:
                    neighbor_idx = positions_dict[neighbor_pos]
                    edges.append([pos_idx, neighbor_idx])
        
        if edges:
            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
            return edge_index
        else:
            return torch.tensor([[], []], dtype=torch.long)
    
    def build_graph_from_dataframe(self, df):
        """
        ä»DataFrameæ„å»ºå›¾ï¼ˆåŒ…å«LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰- æ ¸å¿ƒä¿®æ”¹æ–°å¢èŠ‚ç‚¹æ ‡ç­¾
        """
        # è·å–ä½ç½®
        positions = list(zip(df['position_row'].values, df['position_col'].values))
        
        # æ„å»ºè¾¹
        edge_index = self.build_8_neighbor_edges(positions)
        
        # å‡†å¤‡èŠ‚ç‚¹ç‰¹å¾
        if self.is_fitted and self.feature_columns_to_scale and self.all_feature_columns:
            # å¯¹éœ€è¦æ ‡å‡†åŒ–çš„ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
            features_to_scale = df[self.feature_columns_to_scale].values
            features_scaled = self.scaler.transform(features_to_scale)
            
            # è·å–CNNç‰¹å¾
            cnn_features = df[self.cnn_feature_columns].values if self.cnn_feature_columns else np.array([])
            
            # åˆå¹¶ç‰¹å¾
            X_final = np.zeros((len(df), len(self.all_feature_columns)))
            
            # æ‰¾åˆ°ç‰¹å¾ä½ç½®
            scale_indices = [self.all_feature_columns.index(col) 
                           for col in self.feature_columns_to_scale]
            cnn_indices = [self.all_feature_columns.index(col) 
                         for col in self.cnn_feature_columns] if self.cnn_feature_columns else []
            
            # å¡«å…¥æ ‡å‡†åŒ–ç‰¹å¾
            for i, idx in enumerate(scale_indices):
                X_final[:, idx] = features_scaled[:, i]
            
            # å¡«å…¥CNNç‰¹å¾
            for i, idx in enumerate(cnn_indices):
                X_final[:, idx] = cnn_features[:, i]
        else:
            # å¦‚æœæ²¡æœ‰æ‹Ÿåˆæ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾
            X_final = df[self.all_feature_columns].values if self.all_feature_columns else np.zeros((len(df), 0))
        
        # æ­£ç¡®è½¬æ¢ä¸ºPyTorchå¼ é‡
        x = torch.tensor(X_final, dtype=torch.float)
        pos = torch.tensor(positions, dtype=torch.long)
        
        # ========== æ ¸å¿ƒä¿®æ”¹1ï¼šæ„å»ºèŠ‚ç‚¹çº§æ ‡ç­¾ï¼ˆæ¯ä¸ªèŠ‚ç‚¹å¯¹åº”è‡ªèº«çš„grid_gdpå’Œgrid_gdp_logï¼‰ ==========
        grid_gdp = df['grid_gdp'].values.reshape(-1, 1)  # [num_nodes, 1]
        grid_gdp_log = df['grid_gdp_log'].values.reshape(-1, 1)  # [num_nodes, 1]
        y_node = torch.tensor(np.hstack([grid_gdp, grid_gdp_log]), dtype=torch.float)  # [num_nodes, 2]
        
        # åŸæœ‰ï¼šæ„å»ºå›¾çº§å…¨å±€æ ‡ç­¾ï¼ˆæ•´å›¾GDPæ€»å’Œï¼Œæ­¤å¤„æš‚ä¸è®¾ç½®ï¼Œç•™å¾…å›¾å—ç”Ÿæˆæ—¶å¤„ç†ï¼‰
        y = torch.tensor([[0.0, 0.0]], dtype=torch.float)
        
        # æ­£ç¡®åˆ›å»º Data å¯¹è±¡ï¼ˆæ–°å¢ y_node èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰
        graph_data = Data(
            x=x,
            edge_index=edge_index,
            y=y,  # å›¾çº§å…¨å±€æ ‡ç­¾ï¼ˆåç»­æ›´æ–°ï¼‰
            y_node=y_node,  # æ–°å¢ï¼šèŠ‚ç‚¹çº§æ ‡ç­¾ [num_nodes, 2]
            pos=pos,
            num_nodes=len(df)
        )
        
        # åˆå¹¶LapPEç¼–ç 
        graph_data = self.merge_lap_pe_with_node_features(graph_data)
        
        return graph_data
    
    # ==================== å›¾å—ç”Ÿæˆéƒ¨åˆ†ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šä¿ç•™å›¾å—å†…èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰ ====================
    def extract_patch_from_graph(self, graph_data, start_row, start_col, min_nodes_threshold):
        """
        ä»å›¾ä¸­æå–æŒ‡å®šä½ç½®çš„å›¾å—ï¼ˆä¿ç•™LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰- ä¼˜åŒ–LapPEæå–
        """
        # è·å–ä½ç½®ä¿¡æ¯
        positions = graph_data.pos.numpy()
        
        # é€‰æ‹©åœ¨å›¾å—èŒƒå›´å†…çš„èŠ‚ç‚¹ï¼ˆåŸºäºå®é™…åæ ‡ï¼‰
        mask = ((positions[:, 0] >= start_row) & 
                (positions[:, 0] < start_row + self.patch_size) &
                (positions[:, 1] >= start_col) & 
                (positions[:, 1] < start_col + self.patch_size))
        
        node_indices = np.where(mask)[0]
        
        # è¿‡æ»¤å¤ªå°çš„å›¾å—
        if len(node_indices) < min_nodes_threshold:
            return None, 0.0, 0.0
        
        # æå–åŸºç¡€å­å›¾æ•°æ®
        x_patch = graph_data.x[node_indices]
        pos_patch = graph_data.pos[node_indices]
        
        # æ ¸å¿ƒä¿®æ”¹2ï¼šæå–å›¾å—å†…çš„èŠ‚ç‚¹çº§æ ‡ç­¾
        y_node_patch = graph_data.y_node[node_indices]  # [num_patch_nodes, 2]
        
        # é‡æ–°è®¡ç®—å­å›¾çš„è¾¹è¿æ¥
        positions_patch = [(int(pos[0]), int(pos[1])) for pos in pos_patch]
        edge_index_patch = self.build_8_neighbor_edges(positions_patch)
        
        # è®¡ç®—å›¾å—æ€»GDPï¼ˆåŸå§‹å€¼ï¼‰- åŸºäºèŠ‚ç‚¹çº§æ ‡ç­¾æ±‚å’Œï¼ˆæ›´å‡†ç¡®ï¼‰
        patch_gdp = float(y_node_patch[:, 0].sum().item())
        
        # è®¡ç®—å›¾å—log(1+GDP) - ä½¿ç”¨torchï¼Œé¿å…ç±»å‹æ··æ·†
        patch_log_gdp = float(torch.log1p(torch.tensor(patch_gdp, dtype=torch.float)).item())
        
        # ========== ä¼˜åŒ–ç‚¹ï¼šç›´æ¥æå–å¤§å›¾çš„LapPEå­é›†ï¼Œæ— éœ€é‡æ–°è®¡ç®— ==========
        lap_pe_patch = graph_data.lap_pe[node_indices] if hasattr(graph_data, 'lap_pe') else torch.zeros((len(node_indices), self.lap_pe_k), dtype=torch.float)
        
        # ç›´æ¥åˆ›å»ºå›¾å—Dataå¯¹è±¡ï¼ŒåŒ…å«LapPEå­é›†
        patch_graph = Data(
            x=x_patch,
            edge_index=edge_index_patch,
            pos=pos_patch,
            num_nodes=len(node_indices),
            lap_pe=lap_pe_patch,  # ç›´æ¥èµ‹å€¼æå–çš„LapPEå­é›†
            y=torch.tensor([[patch_gdp, patch_log_gdp]], dtype=torch.float),
            y_node=y_node_patch
        )
        
        # ç§»é™¤å†—ä½™çš„LapPEé‡æ–°è®¡ç®—é€»è¾‘
        if patch_graph.num_nodes <= 0:
            return None, 0.0, 0.0
        
        return patch_graph, patch_gdp, patch_log_gdp
    
    def generate_patches_for_county(self, graph_data, county_name, stride=None, 
                                  min_nodes_threshold=5):
        """
        ä¸ºå¿å›¾ç”Ÿæˆæ»‘åŠ¨çª—å£å›¾å—ï¼ˆä¿ç•™LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰
        """
        if stride is None:
            stride = self.patch_size
        
        patches = []
        patch_gdps = []
        
        # è·å–å›¾çš„è¾¹ç•Œï¼ˆå®é™…åæ ‡èŒƒå›´ï¼‰
        positions = graph_data.pos.numpy()
        if len(positions) == 0:
            return patches, patch_gdps
        
        # è®¡ç®—å®é™…åæ ‡èŒƒå›´
        min_row = int(positions[:, 0].min())
        max_row = int(positions[:, 0].max())
        min_col = int(positions[:, 1].min())
        max_col = int(positions[:, 1].max())
        
        # print(f"  {county_name}: åæ ‡èŒƒå›´: è¡Œ[{min_row}-{max_row}], åˆ—[{min_col}-{max_col}], æ€»èŠ‚ç‚¹æ•°: {len(positions)}")
        
        # è®¡ç®—å›¾å—æ•°é‡ä¼°è®¡
        total_rows = max_row - min_row + 1
        total_cols = max_col - min_col + 1
        
        # è®¡ç®—å›¾å—æ•°é‡ä¼°è®¡
        num_patches_h = max(1, total_rows // stride)
        num_patches_w = max(1, total_cols // stride)
        estimated_patches = num_patches_h * num_patches_w
        
        # åŠ¨æ€é˜ˆå€¼è®¡ç®—
        avg_nodes_in_patch = len(positions) / estimated_patches if estimated_patches > 0 else 0
        
        # æ›´åˆç†çš„é˜ˆå€¼è®¡ç®—ï¼šè€ƒè™‘ä¸åŒå¤§å°çš„å¿
        if avg_nodes_in_patch > 100:
            # å¤§å‹å¿ï¼šé˜ˆå€¼è®¾ä¸ºå¹³å‡èŠ‚ç‚¹æ•°çš„1/5
            dynamic_threshold = max(min_nodes_threshold, int(avg_nodes_in_patch / 5))
        elif avg_nodes_in_patch > 50:
            # ä¸­å‹å¿ï¼šé˜ˆå€¼è®¾ä¸ºå¹³å‡èŠ‚ç‚¹æ•°çš„1/3
            dynamic_threshold = max(min_nodes_threshold, int(avg_nodes_in_patch / 3))
        else:
            # å°å‹å¿ï¼šä½¿ç”¨æœ€å°é˜ˆå€¼
            dynamic_threshold = min_nodes_threshold
        
        # é™åˆ¶æœ€å¤§é˜ˆå€¼ä¸è¶…è¿‡30
        dynamic_threshold = min(dynamic_threshold, 30)
        
        # print(f"  {county_name}: ä¼°è®¡å›¾å—æ•°: {estimated_patches}, å¹³å‡èŠ‚ç‚¹/å›¾å—: {avg_nodes_in_patch:.1f}, åŠ¨æ€é˜ˆå€¼: {dynamic_threshold}")
        
        # æ»‘åŠ¨çª—å£ï¼šä»å®é™…æœ€å°åæ ‡å¼€å§‹ï¼Œåˆ°æœ€å¤§åæ ‡ç»“æŸ
        for start_row in range(min_row, max_row + 1, stride):
            for start_col in range(min_col, max_col + 1, stride):
                patch_graph, patch_gdp, patch_log_gdp = self.extract_patch_from_graph(
                    graph_data, start_row, start_col, dynamic_threshold
                )
                
                if patch_graph is not None and patch_graph.num_nodes > 0:
                    patches.append(patch_graph)
                    patch_gdps.append(patch_gdp)  # ä¿å­˜åŸå§‹GDPå€¼
                    self.patch_county_mapping.append(county_name)
        
        return patches, patch_gdps
    
    # ==================== ä¸»æµç¨‹ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼Œæ— ä¿®æ”¹ï¼‰ ====================
    def build_graph_dataset(self, features_dir, output_dir=None, stride=None, 
                       max_counties=None, random_patches=False, min_nodes_threshold=5,
                       target_county=None):  # æ–°å¢ï¼štarget_countyå‚æ•°ï¼ŒæŒ‡å®šè¦å¤„ç†çš„å¿
        """
        æ„å»ºå›¾æ•°æ®é›†ä¸»æµç¨‹ï¼ˆåŒ…å«LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰
        æ–°å¢ï¼štarget_countyå‚æ•°ï¼Œæ”¯æŒåªå¤„ç†æŒ‡å®šå¿ï¼ˆæ¨ç†æ—¶ç”¨ï¼‰
        """
        # æ¸…ç©ºä¹‹å‰çš„æ˜ å°„
        self.patch_county_mapping = []
        
        # 1. åŠ è½½GDPæ•°æ®
        gdp_dict = self.load_county_gdp_dict()
        
        # 2. è·å–ç‰¹å¾æ–‡ä»¶
        feature_files = [f for f in os.listdir(features_dir) 
                        if f.endswith('_features.csv')]
        print(f"æ‰¾åˆ°ç‰¹å¾æ–‡ä»¶: {len(feature_files)} ä¸ª")
        
        # 3. åŒ¹é…å¿å
        self.county_gdp_dict = self.match_county_names(feature_files, gdp_dict)
        
        if not self.county_gdp_dict:
            print("âŒ æ²¡æœ‰åŒ¹é…çš„å¿æ•°æ®")
            return None
        
        # ========== æ–°å¢ï¼šåªä¿ç•™æŒ‡å®šå¿çš„æ•°æ®ï¼ˆæ¨ç†æ—¶ç”¨ï¼‰ ==========
        if target_county is not None:
            if target_county in self.county_gdp_dict:
                # åªä¿ç•™æŒ‡å®šå¿
                self.county_gdp_dict = {target_county: self.county_gdp_dict[target_county]}
                print(f"ğŸ” ä»…å¤„ç†æŒ‡å®šå¿: {target_county}")
            else:
                raise ValueError(f"âŒ æŒ‡å®šçš„å¿ {target_county} ä¸åœ¨åŒ¹é…çš„å¿åˆ—è¡¨ä¸­")
        
        # 4. æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
        self.fit_scaler(features_dir, self.county_gdp_dict)
        
        # 5. æ„å»ºå›¾å’Œå›¾å—
        print(f"\nğŸ”¨ å¼€å§‹æ„å»ºå›¾æ•°æ®é›† (å›¾å—å°ºå¯¸: {self.patch_size}Ã—{self.patch_size}, LapPEç»´åº¦: {self.lap_pe_k})...")
        
        all_patches = []
        all_patch_gdps = []
        
        county_items = list(self.county_gdp_dict.items())
        if max_counties and target_county is None:  # åªæœ‰æ²¡æŒ‡å®šå¿æ—¶ï¼Œæ‰é™åˆ¶max_counties
            county_items = county_items[:max_counties]
            print(f"ä»…å¤„ç†å‰ {max_counties} ä¸ªå¿ç”¨äºè°ƒè¯•")
        
        for county_name, county_gdp in tqdm(county_items, desc="å¤„ç†å„å¿"):
            # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
            df = self.load_and_preprocess_county(features_dir, county_name, county_gdp)
            if df is None:
                continue
            
            # æ„å»ºå®Œæ•´å›¾ï¼ˆåŒ…å«LapPE+èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰
            graph_data = self.build_graph_from_dataframe(df)
            
            # ç”Ÿæˆå›¾å—ï¼ˆåŒ…å«LapPE+èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰
            if random_patches:
                # éšæœºé‡‡æ ·å›¾å—
                patches, patch_gdps = self._generate_random_patches(graph_data, county_name)
            else:
                # æ»‘åŠ¨çª—å£å›¾å—
                patches, patch_gdps = self.generate_patches_for_county(
                    graph_data, county_name, 
                    stride=stride, 
                    min_nodes_threshold=min_nodes_threshold
                )
            
            # æ·»åŠ åˆ°æ€»åˆ—è¡¨
            all_patches.extend(patches)
            all_patch_gdps.extend(patch_gdps)
            
            print(f"  {county_name}: ç”Ÿæˆ {len(patches)} ä¸ªå›¾å—ï¼ˆå‡åŒ…å«LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰")
        
        # 6. åˆ›å»ºæ•°æ®é›†
        dataset = GraphPatchDataset(all_patches, all_patch_gdps)
        
        # 7. æ±‡æ€»ç»Ÿè®¡
        stats = dataset.get_statistics()
        if stats:
            print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
            print(f"   å¤„ç†å¿æ•°: {len(county_items)}")
            print(f"   æ€»å›¾å—æ•°: {stats['num_patches']}")
            print(f"   å¹³å‡èŠ‚ç‚¹æ•°: {stats['avg_nodes']:.1f} Â± {stats['std_nodes']:.1f}")
            print(f"   èŠ‚ç‚¹èŒƒå›´: {stats['min_nodes']} ~ {stats['max_nodes']}")
            print(f"   å¹³å‡è¾¹æ•°: {stats['avg_edges']:.1f}")
            print(f"   GDPèŒƒå›´: {stats['min_gdp']:.2f} ~ {stats['max_gdp']:.2f} ä¸‡å…ƒ")
            print(f"   log(1+GDP)èŒƒå›´: {stats['min_log_gdp']:.4f} ~ {stats['max_log_gdp']:.4f}")
            print(f"   ç‰¹å¾ç»´åº¦: {stats['feature_dim']}")
            print(f"   åŒ…å«LapPEç¼–ç : {stats['has_lappe']}ï¼ˆç»´åº¦: {self.lap_pe_k}ï¼‰")
            print(f"   åŒ…å«èŠ‚ç‚¹çº§æ ‡ç­¾: {stats['has_node_labels']}ï¼ˆå¹³å‡èŠ‚ç‚¹GDP: {stats['avg_node_gdp']:.2f}ï¼‰")
        
        # 8. ä¿å­˜æ•°æ®ï¼ˆå¯é€‰ï¼Œä¿ç•™LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰
        if output_dir and stats:
            self._save_dataset(dataset, output_dir)
        
        return dataset
    
    def _generate_random_patches(self, graph_data, county_name, num_patches=10):
        """éšæœºé‡‡æ ·å›¾å—ï¼ˆç”¨äºè°ƒè¯•ï¼Œä¿ç•™LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰- ä¿®å¤è¯­æ³•é”™è¯¯"""
        patches = []
        patch_gdps = []
        
        positions = graph_data.pos.numpy()
        if len(positions) == 0:
            return patches, patch_gdps
        
        min_row = int(positions[:, 0].min())
        max_row = int(positions[:, 0].max())
        min_col = int(positions[:, 1].min())
        max_col = int(positions[:, 1].max())
        
        for _ in range(num_patches):
            # éšæœºèµ·å§‹ä½ç½®ï¼ˆç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œï¼‰
            start_row = np.random.randint(min_row, max(max_row - self.patch_size + 1, min_row + 1))
            start_col = np.random.randint(min_col, max(max_col - self.patch_size + 1, min_col + 1))
            
            # ä½¿ç”¨æœ€å°é˜ˆå€¼è¿›è¡Œéšæœºé‡‡æ ·
            patch_graph, patch_gdp, patch_log_gdp = self.extract_patch_from_graph(
                graph_data, start_row, start_col, min_nodes_threshold=3
            )
            
            if patch_graph is not None and patch_graph.num_nodes > 0:
                patches.append(patch_graph)
                patch_gdps.append(patch_gdp)
                self.patch_county_mapping.append(county_name)
        
        return patches, patch_gdps
    
    def _save_dataset(self, dataset, output_dir):
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿®å¤æ ¸å¿ƒï¼šå…ˆåˆ¤æ–­scaleræ˜¯å¦æœ‰mean_å±æ€§ï¼Œæ²¡æœ‰åˆ™èµ‹å€¼None
        scaler_mean = self.scaler.mean_ if hasattr(self.scaler, 'mean_') else None
        scaler_scale = self.scaler.scale_ if hasattr(self.scaler, 'scale_') else None
        
        # ä¿å­˜ä¸ºPyG Datasetæ ¼å¼
        dataset_path = os.path.join(output_dir, 'graph_patches_with_lappe_and_node_labels.pt')
        torch.save({
            'dataset': dataset,
            'patch_county_mapping': self.patch_county_mapping,
            'patch_size': self.patch_size,
            'lap_pe_k': self.lap_pe_k,
            'scaler': self.scaler,
            'feature_columns': {
                'all': self.all_feature_columns,
                'to_scale': self.feature_columns_to_scale,
                'cnn': self.cnn_feature_columns
            },
            # ç”¨åˆ¤æ–­åçš„å˜é‡ï¼Œé¿å…æŠ¥é”™
            'scaler_mean': scaler_mean,
            'scaler_scale': scaler_scale,
            'scaler_var': self.scaler.var_ if hasattr(self.scaler, 'var_') else None,
            'scaler_n_samples_seen': self.scaler.n_samples_seen_ if hasattr(self.scaler, 'n_samples_seen_') else None
        }, dataset_path)
        
        # å•ç‹¬ä¿å­˜scalerçš„éƒ¨åˆ†ä¹ŸåŠ åˆ¤æ–­
        scaler_path = os.path.join(output_dir, 'scaler.pth')
        torch.save({
            'scaler': self.scaler,
            'feature_columns_to_scale': self.feature_columns_to_scale,
            'scaler_mean': scaler_mean,
            'scaler_scale': scaler_scale,
            'scaler_var': self.scaler.var_ if hasattr(self.scaler, 'var_') else None,
            'scaler_n_samples_seen': self.scaler.n_samples_seen_ if hasattr(self.scaler, 'n_samples_seen_') else None
        }, scaler_path)
        
        # ç»Ÿè®¡ä¿¡æ¯ä¿å­˜ä¸å˜
        stats = dataset.get_statistics()
        import json
        stats_path = os.path.join(output_dir, 'dataset_statistics_with_node_labels.json')
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: {dataset_path}")
        print(f"âœ… æ ‡å‡†åŒ–å™¨å·²å•ç‹¬ä¿å­˜åˆ°: {scaler_path}")
        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_path}")


class GraphDataLoaderManager:
    """æ•°æ®åŠ è½½å™¨ç®¡ç†å™¨ï¼ˆæ”¯æŒåŒ…å«LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾çš„å›¾æ•°æ®ï¼‰"""
    
    def __init__(self, batch_size=32, num_workers=0):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
        
        Args:
            batch_size: æ‰¹å¤§å°
            num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
        """
        self.batch_size = batch_size
        self.num_workers = num_workers
        
    def create_data_loaders(self, train_dataset, val_dataset=None, test_dataset=None, 
                      shuffle_train=True):
        """
        åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒLapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾æ•°æ®ï¼‰
        æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨PyG DataLoaderï¼Œè‡ªåŠ¨æ‹¼æ¥Batchå¯¹è±¡ï¼Œé¿å…tupleæŠ¥é”™
        """
        data_loaders = {}
        
        # åˆ›å»ºè®­ç»ƒé›†åŠ è½½å™¨ï¼ˆä½¿ç”¨PyG DataLoaderï¼Œè‡ªåŠ¨å¤„ç†å›¾æ‰¹æ¬¡ï¼‰
        train_loader = PyGDataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=shuffle_train,
            num_workers=self.num_workers,
            pin_memory=torch.cuda.is_available()  # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦ä½¿ç”¨pin_memory
        )
        data_loaders['train'] = train_loader
        print(f"âœ… è®­ç»ƒé›†åŠ è½½å™¨: {len(train_loader)} æ‰¹")
        
        # åˆ›å»ºéªŒè¯é›†åŠ è½½å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if val_dataset is not None and len(val_dataset) > 0:
            val_loader = PyGDataLoader(
                val_dataset,
                batch_size=self.batch_size,
                shuffle=False,
                num_workers=self.num_workers,
                pin_memory=torch.cuda.is_available()
            )
            data_loaders['val'] = val_loader
            print(f"âœ… éªŒè¯é›†åŠ è½½å™¨: {len(val_loader)} æ‰¹")
        
        # åˆ›å»ºæµ‹è¯•é›†åŠ è½½å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if test_dataset is not None and len(test_dataset) > 0:
            test_loader = PyGDataLoader(
                test_dataset,
                batch_size=self.batch_size,
                shuffle=False,
                num_workers=self.num_workers,
                pin_memory=torch.cuda.is_available()
            )
            data_loaders['test'] = test_loader
            print(f"âœ… æµ‹è¯•é›†åŠ è½½å™¨: {len(test_loader)} æ‰¹")
        
        return data_loaders
    
    def get_batch_statistics(self, data_loader, max_batches=10):
        """
        è·å–æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ - æ­£ç¡®å¤„ç†PyG Batchå¯¹è±¡ï¼ˆåŒ…å«LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰
        """
        batch_sizes = []
        node_counts_per_graph = []
        edge_counts_per_graph = []
        gdp_values = []
        log_gdp_values = []
        lap_pe_dims = []
        
        # æ–°å¢ï¼šèŠ‚ç‚¹çº§æ ‡ç­¾ç»Ÿè®¡
        node_gdp_batch_list = []
        
        for i, batch in enumerate(data_loader):
            if i >= max_batches:
                break
            
            # PyG DataLoaderè¿”å›Batchå¯¹è±¡ï¼Œè‡ªåŠ¨æ‹¼æ¥å¤šä¸ªå›¾
            if hasattr(batch, 'batch') and batch.batch is not None:
                # è·å–æ‰¹æ¬¡ä¸­çš„å›¾æ•°é‡
                batch_size = int(batch.batch.max().item()) + 1 if len(batch.batch) > 0 else 1
                batch_sizes.append(batch_size)
                
                # ä»Batchå¯¹è±¡ä¸­æå–æ¯ä¸ªå›¾çš„èŠ‚ç‚¹æ•°
                if hasattr(batch, '__num_nodes__') and batch.__num_nodes__:
                    # æ–°ç‰ˆæœ¬PyG
                    num_nodes_list = batch.__num_nodes__
                    if isinstance(num_nodes_list, list):
                        node_counts_per_graph.extend(num_nodes_list)
                else:
                    # æ—§ç‰ˆæœ¬æˆ–æ‰‹åŠ¨è®¡ç®—
                    for graph_idx in range(batch_size):
                        node_count = (batch.batch == graph_idx).sum().item() if len(batch.batch) > 0 else 0
                        node_counts_per_graph.append(node_count)
                
                # ä»Batchå¯¹è±¡ä¸­æå–æ¯ä¸ªå›¾çš„è¾¹æ•°
                if hasattr(batch, 'edge_index') and batch.edge_index is not None and batch.edge_index.shape[1] > 0:
                    # éœ€è¦æ ¹æ®batch.batchåˆ†ç¦»æ¯ä¸ªå›¾çš„è¾¹
                    edge_batch = batch.batch[batch.edge_index[0]]  # æ¯æ¡è¾¹çš„èµ·ç‚¹å±äºå“ªä¸ªå›¾
                    for graph_idx in range(batch_size):
                        edge_count = (edge_batch == graph_idx).sum().item()
                        edge_counts_per_graph.append(edge_count)
                
                # è·å–GDPå€¼
                if hasattr(batch, 'y') and batch.y is not None and batch.y.dim() >= 2:
                    if batch.y.dim() == 2:  # [batch_size, 2]
                        gdp_values.extend(batch.y[:, 0].tolist())
                        log_gdp_values.extend(batch.y[:, 1].tolist())
                
                # è®°å½•LapPEç»´åº¦
                if hasattr(batch, 'lap_pe') and batch.lap_pe is not None:
                    lap_pe_dims.append(batch.lap_pe.shape[-1])
                
                # æ–°å¢ï¼šè®°å½•èŠ‚ç‚¹çº§GDP
                if hasattr(batch, 'y_node') and batch.y_node is not None:
                    node_gdp_batch_list.extend(batch.y_node[:, 0].tolist())
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¢åŠ å®¹é”™ï¼Œé¿å…ç©ºåˆ—è¡¨æŠ¥é”™ï¼‰
        def safe_mean(arr):
            return float(np.mean(arr)) if arr and not np.isnan(np.mean(arr)) else 0.0
        
        def safe_std(arr):
            return float(np.std(arr)) if arr and not np.isnan(np.std(arr)) else 0.0
        
        def safe_min(arr):
            return float(np.min(arr)) if arr else 0.0
        
        def safe_max(arr):
            return float(np.max(arr)) if arr else 0.0
        
        stats = {
            'avg_batch_size': safe_mean(batch_sizes),
            'std_batch_size': safe_std(batch_sizes),
            'avg_nodes': safe_mean(node_counts_per_graph),
            'std_nodes': safe_std(node_counts_per_graph),
            'avg_edges': safe_mean(edge_counts_per_graph),
            'std_edges': safe_std(edge_counts_per_graph),
            'avg_gdp': safe_mean(gdp_values),
            'std_gdp': safe_std(gdp_values),
            'avg_log_gdp': safe_mean(log_gdp_values),
            'std_log_gdp': safe_std(log_gdp_values),
            'min_gdp': safe_min(gdp_values),
            'max_gdp': safe_max(gdp_values),
            'avg_lap_pe_dim': safe_mean(lap_pe_dims),
            'has_lappe': bool(len(lap_pe_dims) > 0),
            # æ–°å¢ï¼šèŠ‚ç‚¹çº§æ ‡ç­¾ç»Ÿè®¡
            'avg_node_gdp_in_batch': safe_mean(node_gdp_batch_list),
            'has_node_labels': bool(len(node_gdp_batch_list) > 0)
        }
        
        return stats


# ========== ä¸»ç¨‹åºè¿è¡Œ ==========
if __name__ == "__main__":
    

    # åˆå§‹åŒ–å›¾æ•°æ®æ„å»ºå™¨ï¼ˆæŒ‡å®šLapPEç»´åº¦ï¼‰
    builder = GraphDataBuilder(
        gdp_file_path='./dataset/åˆ†å¿GDPç»Ÿè®¡.xlsx',
        patch_size=12,  
        lap_pe_k=12  # LapPEç¼–ç ç»´åº¦ä¸º12ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
    )
    
    # æ£€æŸ¥æ•°æ®é›†ç›®å½•æ˜¯å¦å­˜åœ¨
    features_dir = './dataset/extracted_features_90'
    if not os.path.exists(features_dir):
        os.makedirs(features_dir, exist_ok=True)
        print(f"âš ï¸  ç‰¹å¾ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»º: {features_dir}")
    
    # æ„å»ºæ•°æ®é›†
    output_dir = './dataset/graph_data_with_lappe_and_node_labels'
    
    # æ„å»ºå®Œæ•´æ•°æ®é›†ï¼ˆåŒ…å«LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰
    dataset = builder.build_graph_dataset(
        features_dir=features_dir,
        output_dir=output_dir,
        stride=6,  # æ— é‡å 
        max_counties=90,  # ä»…å¤„ç†å‰10ä¸ªå¿ç”¨äºæµ‹è¯•
        random_patches=False,
        min_nodes_threshold=30  # æœ€å°èŠ‚ç‚¹æ•°é˜ˆå€¼
    )
    
    if dataset is not None and len(dataset) > 0:
        
        # ========== åŸæœ‰é€»è¾‘ï¼šæ•°æ®é›†åˆ’åˆ† ==========
        print("\n" + "="*50)
        print("æ•°æ®é›†åˆ’åˆ†ï¼ˆä¿ç•™LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰")
        print("="*50)
        
        # æ–¹å¼1: æŒ‰å¿åˆ’åˆ†ï¼ˆæ¨èï¼Œé˜²æ­¢æ•°æ®æ³„éœ²ï¼‰
        train_dataset, val_dataset, test_dataset = dataset.split_by_county(
            builder.patch_county_mapping,
            test_size=0.2,
            random_state=42
        )
        
        # ========== åŸæœ‰é€»è¾‘ï¼šåˆ›å»ºæ•°æ®åŠ è½½å™¨ ==========
        print("\n" + "="*50)
        print("åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒLapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰")
        print("="*50)
        
        loader_manager = GraphDataLoaderManager(
            batch_size=8,  # è¾ƒå°çš„æ‰¹å¤§å°ï¼Œå› ä¸ºå›¾çš„å¤§å°ä¸åŒ
            num_workers=0  # åœ¨Windowsä¸Šè®¾ç½®ä¸º0ï¼ŒLinuxä¸Šå¯ä»¥è®¾ä¸º2-4
        )
        
        data_loaders = loader_manager.create_data_loaders(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=test_dataset,
            shuffle_train=True
        )
        
        # ========== åŸæœ‰é€»è¾‘ï¼šæ£€æŸ¥æ‰¹æ¬¡ç»Ÿè®¡ ==========
        print("\n" + "="*50)
        print("æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«LapPEç¼–ç +èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰")
        print("="*50)
        
        for split, loader in data_loaders.items():
            stats = loader_manager.get_batch_statistics(loader, max_batches=5)
            print(f"\n{split} é›†:")
            print(f"  å¹³å‡æ‰¹æ¬¡å¤§å°: {stats['avg_batch_size']:.1f} Â± {stats['std_batch_size']:.1f}")
            print(f"  å¹³å‡èŠ‚ç‚¹æ•°: {stats['avg_nodes']:.1f} Â± {stats['std_nodes']:.1f}")
            print(f"  å¹³å‡è¾¹æ•°: {stats['avg_edges']:.1f} Â± {stats['std_edges']:.1f}")
            print(f"  GDPèŒƒå›´: {stats['min_gdp']:.2f} ~ {stats['max_gdp']:.2f} ä¸‡å…ƒ")
            print(f"  å¹³å‡log(1+GDP): {stats['avg_log_gdp']:.4f} Â± {stats['std_log_gdp']:.4f}")
            print(f"  åŒ…å«LapPEç¼–ç : {stats['has_lappe']}ï¼Œå¹³å‡ç»´åº¦: {stats['avg_lap_pe_dim']:.1f}")
            print(f"  åŒ…å«èŠ‚ç‚¹çº§æ ‡ç­¾: {stats['has_node_labels']}ï¼Œå¹³å‡èŠ‚ç‚¹GDP: {stats['avg_node_gdp_in_batch']:.2f}")
        
        # ========== åŸæœ‰é€»è¾‘ï¼šæ£€æŸ¥ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼ˆéªŒè¯èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰ ==========
        print("\n" + "="*50)
        print("æ£€æŸ¥ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡ï¼ˆéªŒè¯èŠ‚ç‚¹çº§æ ‡ç­¾ï¼‰")
        print("="*50)
        
        train_loader = data_loaders.get('train')
        if train_loader:
            for batch in train_loader:
                # PyG Batchå¯¹è±¡è‡ªåŠ¨æ‹¼æ¥ï¼Œè·å–æ‰¹æ¬¡ä¿¡æ¯
                print(f"æ‰¹æ¬¡ç±»å‹: {type(batch)}")
                print(f"æ‰¹æ¬¡èŠ‚ç‚¹ç‰¹å¾å½¢çŠ¶: {batch.x.shape}")
                print(f"æ‰¹æ¬¡è¾¹ç´¢å¼•å½¢çŠ¶: {batch.edge_index.shape}")
                print(f"æ‰¹æ¬¡å›¾çº§æ ‡ç­¾å½¢çŠ¶: {batch.y.shape}")
                if hasattr(batch, 'y_node'):
                    print(f"æ‰¹æ¬¡èŠ‚ç‚¹çº§æ ‡ç­¾å½¢çŠ¶: {batch.y_node.shape}")
                if hasattr(batch, 'lap_pe'):
                    print(f"æ‰¹æ¬¡LapPEç¼–ç å½¢çŠ¶: {batch.lap_pe.shape}")
                print(f"æ‰¹æ¬¡åŒ…å«å›¾æ•°é‡: {int(batch.batch.max().item()) + 1 if len(batch.batch) > 0 else 1}")
                
                # éªŒè¯ç¬¬ä¸€ä¸ªå›¾çš„èŠ‚ç‚¹çº§æ ‡ç­¾æ±‚å’Œ
                if hasattr(batch, 'y_node') and hasattr(batch, 'batch') and len(batch.batch) > 0:
                    first_graph_mask = (batch.batch == 0)
                    first_graph_node_gdp = batch.y_node[first_graph_mask, 0]
                    first_graph_node_gdp_sum = first_graph_node_gdp.sum().item()
                    first_graph_patch_gdp = batch.y[0, 0].item()
                    print(f"  ç¬¬ä¸€ä¸ªå›¾èŠ‚ç‚¹çº§GDPæ€»å’Œ: {first_graph_node_gdp_sum:.2f}")
                    print(f"  ç¬¬ä¸€ä¸ªå›¾å›¾å—çº§GDP: {first_graph_patch_gdp:.2f}ï¼ˆè¯¯å·®: {abs(first_graph_node_gdp_sum - first_graph_patch_gdp):.4f}ï¼‰")
                break
        
        # # ========== æ–°å¢ï¼šæ¼”ç¤ºå¦‚ä½•åŠ è½½ä¿å­˜çš„scaler.pth ==========
        # print("\n" + "="*50)
        # print("æ¼”ç¤ºåŠ è½½ä¿å­˜çš„scaler.pth")
        # print("="*50)
        # scaler_path = os.path.join(output_dir, 'scaler.pth')
        # if os.path.exists(scaler_path):
        #     scaler_data = torch.load(scaler_path, weights_only=False)
        #     loaded_scaler = scaler_data['scaler']
        #     print(f"âœ… æˆåŠŸåŠ è½½scaler.pth")
        #     print(f"  scalerå‡å€¼: {loaded_scaler.mean_[:5]}...")  # æ‰“å°å‰5ä¸ªå‡å€¼
        #     print(f"  scalerç¼©æ”¾: {loaded_scaler.scale_[:5]}...")  # æ‰“å°å‰5ä¸ªç¼©æ”¾å€¼
        #     print(f"  æ ‡å‡†åŒ–ç‰¹å¾åˆ—: {scaler_data['feature_columns_to_scale'][:5]}...")  # æ‰“å°å‰5ä¸ªç‰¹å¾åˆ—
        # else:
        #     print(f"âŒ æœªæ‰¾åˆ°scaler.pthæ–‡ä»¶")