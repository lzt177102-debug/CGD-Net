# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
from torch_geometric.loader import DataLoader
from torch_scatter import scatter_add
import numpy as np
import os
import json
import yaml
import argparse
from tqdm import tqdm
import time
from typing import Dict, List, Tuple
import cv2
import matplotlib.pyplot as plt
from osgeo import gdal
import pandas as pd
from osgeo import gdal, osr  # ç¡®ä¿å¯¼å…¥osræ¨¡å—
# å¯¼å…¥æ•°æ®åŠ è½½å™¨ä¸æ¨¡å‹ï¼ˆå¤ç”¨è®­ç»ƒæ—¶çš„ç»„ä»¶ï¼‰
from RSGCN_DataLoader import GraphDataBuilder, GraphDataLoaderManager, GraphPatchDataset
from src.networks.GraphMamba import GraphGDP
from src.networks.GCN import GraphGDP_GCN
from src.networks.GraphGPS import GraphGDP_GraphGPS
from src.networks.Graphormer import GraphGDP_Graphormer
from feature_engineering import extract_features_from_paired_dataset

# ==================== å·¥å…·å‡½æ•°ï¼ˆæ— æ–°å¢ï¼Œä»…é€‚é…ä¼ªæ ‡ç­¾æå–ï¼‰ ====================
def get_device():
    """è·å–æ¨ç†è®¾å¤‡ï¼ˆä¼˜å…ˆGPUï¼‰"""
    return torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

def makedirs(path):
    """åˆ›å»ºç›®å½•"""
    if not os.path.exists(path):
        os.makedirs(path)
    return path

def load_model(model, checkpoint_path, device, multi_gpu=False):
    """åŠ è½½æ¨ç†æ¨¡å‹ï¼ˆå…¼å®¹å¤šå¡æƒé‡ï¼‰"""
    pretrain = torch.load(checkpoint_path, map_location=device)
    state_dict = pretrain.get('model_state_dict', pretrain.get('state_dict', pretrain))
    
    # ç§»é™¤å¤šå¡å‰ç¼€
    if len(state_dict) > 0 and list(state_dict.keys())[0].startswith('module.'):
        state_dict = {k[len("module."):]: v for k, v in state_dict.items()}
    
    # åŠ è½½æƒé‡
    model.load_state_dict(state_dict, strict=False)
    model = model.to(device)
    model.eval()
    return model

def read_tif(file_path):
    """è¯»å–TIFFæ–‡ä»¶ï¼ˆæ—¥é—´é¥æ„Ÿ/å¤œé—´ç¯å…‰ç­‰ï¼‰ï¼Œè¿”å›æ•°æ®å’Œå®Œæ•´åœ°ç†ä¿¡æ¯ï¼ˆä»¥æ—¥é—´é¥æ„Ÿä¸ºæ ‡å‡†ï¼‰"""
    dataset = gdal.Open(file_path)
    if dataset is None:
        raise FileNotFoundError(f"æ— æ³•æ‰“å¼€TIFFæ–‡ä»¶ï¼š{file_path}")
    data = dataset.ReadAsArray()
    
    # ä¸¥æ ¼è¯»å–æ—¥é—´é¥æ„Ÿçš„å®Œæ•´åœ°ç†ä¿¡æ¯ï¼ˆæ ¸å¿ƒï¼‰
    geo_transform = dataset.GetGeoTransform()
    projection = dataset.GetProjection()
    
    # å¼ºåˆ¶æ ¡éªŒï¼šç¡®ä¿è¯»å–çš„æ˜¯æœ‰å®Œæ•´åœ°ç†ä¿¡æ¯çš„æ—¥é—´é¥æ„Ÿå›¾
    if geo_transform is None or projection is None:
        raise ValueError(f"é”™è¯¯ï¼šæ—¥é—´é¥æ„ŸTIFFæ–‡ä»¶ {file_path} ç¼ºå°‘å…³é”®åœ°ç†ä¿¡æ¯ï¼")
    
    geo_info = {
        'transform': geo_transform,  # å®Œå…¨å¤ç”¨æ—¥é—´é¥æ„Ÿçš„åœ°ç†å˜æ¢
        'projection': projection,    # å®Œå…¨å¤ç”¨æ—¥é—´é¥æ„Ÿçš„æŠ•å½±ä¿¡æ¯
        'width': dataset.RasterXSize,
        'height': dataset.RasterYSize
    }
    dataset = None
    return data, geo_info

def write_tif(data, geo_info, output_path):
    """ä¿å­˜TIFFæ–‡ä»¶ï¼ˆä¸¥æ ¼ä»¥æ—¥é—´é¥æ„Ÿçš„åœ°ç†ä¿¡æ¯ä¸ºæ ‡å‡†ï¼Œå–æ¶ˆNoDataè®¾ç½®ï¼‰"""
    driver = gdal.GetDriverByName('GTiff')
    if len(data.shape) == 2:
        bands = 1
        data = data[np.newaxis, :, :]
    else:
        bands = data.shape[0]
    
    # 1. å…ˆåˆ é™¤å·²æœ‰æ–‡ä»¶ï¼ˆé¿å…GDALç¼“å­˜å¯¼è‡´åœ°ç†ä¿¡æ¯å†™å…¥å¤±è´¥ï¼‰
    if os.path.exists(output_path):
        os.remove(output_path)
    
    # 2. ä¸¥æ ¼æŒ‰ç…§æ—¥é—´é¥æ„Ÿçš„å°ºå¯¸å’Œåœ°ç†ä¿¡æ¯åˆ›å»ºTIFF
    dataset = driver.Create(
        output_path,
        geo_info['width'],          # å¤ç”¨æ—¥é—´é¥æ„Ÿå®½åº¦
        geo_info['height'],         # å¤ç”¨æ—¥é—´é¥æ„Ÿé«˜åº¦
        bands,
        gdal.GDT_Float32            # ä¿æŒå’Œä½ åŸæœ‰ä»£ç ä¸€è‡´çš„ç²¾åº¦
    )
    
    # 3. å¼ºåˆ¶å†™å…¥æ—¥é—´é¥æ„Ÿçš„åœ°ç†ä¿¡æ¯ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰
    dataset.SetGeoTransform(geo_info['transform'])  # å®Œå…¨ç»§æ‰¿æ—¥é—´é¥æ„Ÿçš„åæ ‡å˜æ¢
    dataset.SetProjection(geo_info['projection'])   # å®Œå…¨ç»§æ‰¿æ—¥é—´é¥æ„Ÿçš„æŠ•å½±
    
    # 4. å†™å…¥æ•°æ®ï¼ˆå–æ¶ˆNoDataè®¾ç½®ï¼Œå®Œå…¨æŒ‰ä½ çš„åŸå§‹é€»è¾‘ï¼‰
    for i in range(bands):
        dataset.GetRasterBand(i+1).WriteArray(data[i])
    
    dataset.FlushCache()
    dataset = None
    
    # éªŒè¯ï¼šç¡®ä¿åœ°ç†ä¿¡æ¯å†™å…¥æˆåŠŸ
    verify_ds = gdal.Open(output_path)
    if verify_ds:
        if verify_ds.GetGeoTransform() == geo_info['transform'] and verify_ds.GetProjection() == geo_info['projection']:
            print(f"âœ… TIFFæ–‡ä»¶å·²ä¿å­˜ï¼ˆåœ°ç†ä¿¡æ¯ä¸æ—¥é—´é¥æ„Ÿå®Œå…¨ä¸€è‡´ï¼‰ï¼š{output_path}")
        else:
            print(f"âš ï¸ è­¦å‘Šï¼š{output_path} åœ°ç†ä¿¡æ¯ä¸æ—¥é—´é¥æ„Ÿä¸ä¸€è‡´ï¼")
        verify_ds = None
    else:
        print(f"âŒ é”™è¯¯ï¼š{output_path} ä¿å­˜å¤±è´¥ï¼")

def restore_gdp(log_gdp: np.ndarray) -> np.ndarray:
    """ä»log(1+GDP)è¿˜åŸåŸå§‹GDP"""
    raw_gdp = np.expm1(log_gdp)
    raw_gdp[raw_gdp < 0] = 0
    return raw_gdp

def normalize_to_255(data: np.ndarray) -> np.ndarray:
    """ä»…å¯¹GDPçŸ©é˜µå½’ä¸€åŒ–åˆ°0-255ï¼ˆç”Ÿæˆçƒ­åŠ›å›¾TIFFï¼‰"""
    if np.max(data) == np.min(data):
        return np.zeros_like(data, dtype=np.float32)
    norm_data = (data - np.min(data)) / (np.max(data) - np.min(data)) * 255
    norm_data = np.clip(norm_data, 0, 255)
    return norm_data.astype(np.float32)

# ==================== æ ¸å¿ƒæ¨ç†ç±»ï¼ˆæå–å›¾æ„å»ºé˜¶æ®µçš„ä¼ªæ ‡ç­¾å¹¶ä¿å­˜ï¼‰ ====================
class GraphGDPInferencer:
    """GraphGDPæ¨¡å‹æ¨ç†ç±»ï¼ˆæå–å›¾æ„å»ºé˜¶æ®µç”Ÿæˆçš„ä¼ªæ ‡ç­¾ï¼‰"""
    def __init__(self, config):
        self.config = config
        self.device = get_device()
        self.output_dir = makedirs(config['infer_output_dir'])
        
        # æ‰§è¡Œç‰¹å¾æå–å¹¶è·å–è¡Œåˆ—åˆ†å¼€çš„åˆ†è¾¨ç‡æ¯”ä¾‹
        self.features, self.county_node_sizes = extract_features_from_paired_dataset(
            model_path=self.config['model_path'],
            remote_sensing_dir=self.config['remote_sensing_dir'],
            nl_dir=self.config['nl_dir'],
            landuse_dir=self.config['landuse_dir'],
            population_dir=self.config['population_dir'],
            output_dir=self.config['output_dir'],
            model_name=self.config['model_name'],
            num_processes=1,
            poi_dir=self.config['poi_dir'],
            target_county=self.config['infer_county']
        )
        
        # è·å–å½“å‰æ¨ç†å¿çš„è¡Œåˆ—æ¯”ä¾‹
        if self.config['infer_county'] in self.county_node_sizes:
            self.rs_to_nl_ratio_row, self.rs_to_nl_ratio_col = self.county_node_sizes[self.config['infer_county']]
        else:
            self.rs_to_nl_ratio_row = 30
            self.rs_to_nl_ratio_col = 30
        
        print(f"ğŸ” åˆ†è¾¨ç‡æ¯”ä¾‹ï¼ˆè¡Œåˆ—åˆ†å¼€ï¼‰ï¼š{self.rs_to_nl_ratio_row}Ã—{self.rs_to_nl_ratio_col}")

        # åŠ è½½æ¨¡å‹ã€æ•°æ®é›†ã€scaler
        self.model = self._build_model()
        self.infer_dataset, self.geo_info, self.rs_data = self._build_infer_dataset()
        self.scaler = self._load_scaler()
        
        # ========== æ ¸å¿ƒæ–°å¢ï¼šæå–å›¾æ„å»ºé˜¶æ®µç”Ÿæˆçš„ä¼ªæ ‡ç­¾çŸ©é˜µ ==========
        self.pseudo_label_matrix = self._extract_pseudo_label_from_dataset()
    
    def _build_model(self):
        """æ„å»ºå¹¶åŠ è½½æ¨ç†æ¨¡å‹"""
        # 1. å®šä¹‰æ¨¡å‹æ˜ å°„ï¼ˆæ–°å¢GraphGPSå’ŒGraphormerï¼‰
        model_mapping = {
            "GraphGDP": GraphGDP,
            "GraphGDP_GCN": GraphGDP_GCN,
            "GraphGDP_GraphGPS": GraphGDP_GraphGPS,  # æ–°å¢GraphGPSæ¨¡å‹
            "GraphGDP_Graphormer": GraphGDP_Graphormer  # æ–°å¢Graphormeræ¨¡å‹
        }

        # 2. è·å–å¹¶æ ¡éªŒé…ç½®ä¸­çš„æ¨¡å‹ç±»å‹
        model_cls_name = self.config.get("model_cls", "GraphGDP")  # é»˜è®¤ä½¿ç”¨GraphGDP
        supported_models = list(model_mapping.keys())
        if model_cls_name not in model_mapping:
            raise ValueError(f"âŒ ä»…æ”¯æŒ model_cls={'/'.join(supported_models)}ï¼Œå½“å‰é…ç½®ä¸ºï¼š{model_cls_name}")
        model_cls = model_mapping[model_cls_name]
        print(f"ğŸ“Œ åˆå§‹åŒ–æ¨¡å‹ç±»å‹ï¼š{model_cls_name}")

        # 3. å‡†å¤‡æ¨¡å‹é€šç”¨å‚æ•°
        base_model_kwargs = {
            "channels": self.config['channels'],
            "pe_dim": self.config['pe_dim'],
            "num_layers": self.config['num_layers'],
            "use_rw_pe": self.config['use_rw_pe'],
            "use_lap_pe": self.config['use_lap_pe'],
            "use_grid_pe": self.config['use_grid_pe'],
            "node_dim": self.config['node_dim'],
            "edge_dim": self.config['edge_dim'],
            "if_pool": self.config['if_pool'],
            "pool_type": self.config.get('pool_type', 'add'),
            "drop": self.config['drop'],
            "node_label_dim": self.config['node_label_dim'],
            "global_label_dim": self.config['global_label_dim']
        }

        # 4. ä¸ºä¸åŒæ¨¡å‹è¡¥å……ä¸“å±å‚æ•°
        model_kwargs = base_model_kwargs.copy()

        if model_cls_name == "GraphGDP":
            # GraphGDPï¼ˆMambaç‰ˆï¼‰ä¸“å±å‚æ•°
            model_kwargs.update({
                "model_type": self.config['model_type'],
                "shuffle_ind": self.config['shuffle_ind'],
                "d_state": self.config['d_state'],
                "d_conv": self.config['d_conv'],
                "order_by_degree": self.config['order_by_degree']
            })
        elif model_cls_name == "GraphGDP_GraphGPS":
            # GraphGPSï¼ˆGINE+Performerï¼‰ä¸“å±å‚æ•°
            model_kwargs.update({
                "performer_heads": self.config.get('performer_heads', 4),  # é»˜è®¤4å¤´
                "performer_dim_head": self.config.get('performer_dim_head', 32),  # é»˜è®¤dim_head=32
                "performer_depth": self.config.get('performer_depth', 1)  # Performerå¿…å¡«çš„depthå‚æ•°
            })
        elif model_cls_name == "GraphGDP_Graphormer":
            # Graphormerä¸“å±å‚æ•°
            model_kwargs.update({
                "graphormer_heads": self.config.get('graphormer_heads', 8)  # é»˜è®¤8å¤´æ³¨æ„åŠ›
            })
        # GraphGDP_GCNæ— ä¸“å±å‚æ•°ï¼Œæ— éœ€é¢å¤–è¡¥å……

        # 5. åˆå§‹åŒ–æ¨¡å‹
        model = model_cls(**model_kwargs).to(self.device)
        return model
    
    def _load_scaler(self):
        """åŠ è½½è®­ç»ƒæ—¶çš„scaler"""
        scaler_path = self.config['scaler_path']
        if not os.path.exists(scaler_path):
            raise FileNotFoundError(f"scaleræ–‡ä»¶ä¸å­˜åœ¨ï¼š{scaler_path}")
        scaler = torch.load(scaler_path, map_location='cpu', weights_only=False)
        # å…¼å®¹å•ç‹¬ä¿å­˜çš„scaleræˆ–æ•°æ®é›†é‡Œçš„scaler
        scaler = scaler['scaler'] if 'scaler' in scaler else scaler
        print(f"âœ… scaleråŠ è½½å®Œæˆï¼š{scaler_path}")
        return scaler
    
    def _build_infer_dataset(self):
        """æ„å»ºæŒ‡å®šå¿çš„æ¨ç†æ•°æ®é›†ï¼ˆå¤ç”¨å›¾æ„å»ºé€»è¾‘ï¼‰"""
        print(f"\n===== åŠ è½½æ¨ç†æ•°æ®ï¼ˆ{self.config['infer_county']}ï¼‰ =====")
        # è¯»å–åŸå§‹é¥æ„Ÿå›¾ï¼ˆä»…ç”¨äºè·å–åœ°ç†ä¿¡æ¯ï¼‰
        rs_tif_path = self.config['rs_tif_path']
        rs_data, geo_info = read_tif(rs_tif_path)
        print(f"âœ… åŸå§‹é¥æ„Ÿå›¾åŠ è½½å®Œæˆï¼š{rs_tif_path} (å°ºå¯¸ï¼š{geo_info['height']}Ã—{geo_info['width']})")
        
        # æ„å»ºå›¾æ•°æ®é›†ï¼ˆå¤ç”¨è®­ç»ƒæ—¶çš„GraphDataBuilderï¼‰
        builder = GraphDataBuilder(
            gdp_file_path=self.config['gdp_file_path'],
            patch_size=self.config['patch_size'],
            lap_pe_k=self.config['lap_pe_k']
        )
        
        # æ„å»ºä»…åŒ…å«ç›®æ ‡å¿çš„æ¨ç†æ•°æ®é›†
        dataset = builder.build_graph_dataset(
            features_dir=self.config.get('features_dir', './dataset/extracted_features_90'),
            output_dir=None,  # æ¨ç†æ—¶ä¸ä¿å­˜æ•°æ®é›†
            stride=self.config.get('stride', 6),
            max_counties=None,
            random_patches=False,
            min_nodes_threshold=self.config.get('min_nodes_threshold', 5),
            target_county=self.config['infer_county']  # ä»…å¤„ç†ç›®æ ‡å¿
        )
        
        if len(dataset) == 0:
            raise ValueError(f"æœªæ‰¾åˆ°{self.config['infer_county']}çš„å›¾æ•°æ®")
        
        print(f"âœ… æ¨ç†æ•°æ®é›†åŠ è½½å®Œæˆï¼š{len(dataset)} ä¸ªå›¾å—")
        return dataset, geo_info, rs_data
    
    def _extract_pseudo_label_from_dataset(self):
        """
        ä¿®å¤åï¼šæå–å›¾æ„å»ºé˜¶æ®µçš„ä¼ªæ ‡ç­¾çŸ©é˜µï¼ˆå’Œé¢„æµ‹å€¼åæ ‡æ˜ å°„é€»è¾‘å®Œå…¨å¯¹é½ï¼‰
        """
        print(f"\n===== æå–å›¾æ„å»ºé˜¶æ®µçš„ä¼ªæ ‡ç­¾ï¼ˆ{self.config['infer_county']}ï¼‰ =====")
        
        # åˆå§‹åŒ–ä¼ªæ ‡ç­¾çŸ©é˜µï¼ˆå’Œé¥æ„Ÿå›¾å°ºå¯¸ä¸€è‡´ï¼‰
        pseudo_label_matrix = np.zeros((self.geo_info['height'], self.geo_info['width']), dtype=np.float32)
        
        total_nodes = 0
        valid_fill = 0
        non_zero_label_count = 0
        
        # éå†æ‰€æœ‰å›¾å—ï¼Œæå–ä¼ªæ ‡ç­¾å¹¶å¡«å……åˆ°çŸ©é˜µ
        for patch_idx, patch in enumerate(tqdm(self.infer_dataset, desc="æå–ä¼ªæ ‡ç­¾")):
            # 1. æ ¡éªŒæ ¸å¿ƒå±æ€§æ˜¯å¦å­˜åœ¨ï¼ˆå’ŒGraphDataBuilderè¾“å‡ºå¯¹é½ï¼‰
            if not hasattr(patch, 'pos') or patch.pos is None:
                print(f"âš ï¸ å›¾å— {patch_idx} æ— poså±æ€§ï¼Œè·³è¿‡")
                continue
            if not hasattr(patch, 'y_node') or patch.y_node is None:
                print(f"âš ï¸ å›¾å— {patch_idx} æ— y_nodeå±æ€§ï¼Œè·³è¿‡")
                continue
            
            # 2. è·å–èŠ‚ç‚¹ä½ç½®å’Œä¼ªæ ‡ç­¾ï¼ˆå’ŒGraphDataBuilderç”Ÿæˆçš„æ ¼å¼å¯¹é½ï¼‰
            node_positions = patch.pos.numpy()  # [num_nodes, 2] (ç½‘æ ¼è¡Œ, ç½‘æ ¼åˆ—)
            node_pseudo_labels = patch.y_node[:, 0].numpy()  # å–ç¬¬0åˆ—ï¼šåŸå§‹grid_gdpï¼ˆélogå€¼ï¼‰
            
            # è°ƒè¯•ï¼šæ‰“å°å‰3ä¸ªå›¾å—çš„å…³é”®ä¿¡æ¯
            if patch_idx < 3:
                print(f"\nğŸ“Œ å›¾å— {patch_idx} è°ƒè¯•ä¿¡æ¯ï¼š")
                print(f"   èŠ‚ç‚¹æ•°ï¼š{len(node_positions)}")
                print(f"   ä¼ªæ ‡ç­¾èŒƒå›´ï¼š{node_pseudo_labels.min():.4f} ~ {node_pseudo_labels.max():.4f}")
                print(f"   éé›¶ä¼ªæ ‡ç­¾æ•°ï¼š{np.count_nonzero(node_pseudo_labels)}")
                print(f"   å‰5ä¸ªèŠ‚ç‚¹ä½ç½®ï¼š{node_positions[:5]}")
                print(f"   å‰5ä¸ªä¼ªæ ‡ç­¾å€¼ï¼š{node_pseudo_labels[:5]}")
            
            total_nodes += len(node_positions)
            non_zero_label_count += np.count_nonzero(node_pseudo_labels)
            
            # 3. å¡«å……çŸ©é˜µï¼ˆå…³é”®ä¿®å¤ï¼šå’Œé¢„æµ‹å€¼ç”¨å®Œå…¨ç›¸åŒçš„åæ ‡ç¼©æ”¾é€»è¾‘ï¼‰
            for i, (grid_row, grid_col) in enumerate(node_positions):
                label_value = node_pseudo_labels[i]
                if label_value <= 0:  # è¿‡æ»¤æ— æ•ˆæ ‡ç­¾
                    continue
                
                # æ ¸å¿ƒä¿®å¤ï¼šç½‘æ ¼åæ ‡ â†’ åƒç´ åæ ‡ï¼ˆå’Œé¢„æµ‹å€¼é€»è¾‘ä¸€è‡´ï¼‰
                pixel_row = int(grid_row * self.rs_to_nl_ratio_row)
                pixel_col = int(grid_col * self.rs_to_nl_ratio_col)
                
                # è®¡ç®—åƒç´ å—èŒƒå›´ï¼ˆè¦†ç›–æ•´ä¸ªç½‘æ ¼å¯¹åº”çš„åƒç´ åŒºåŸŸï¼‰
                row_end = min(pixel_row + self.rs_to_nl_ratio_row, self.geo_info['height'])
                col_end = min(pixel_col + self.rs_to_nl_ratio_col, self.geo_info['width'])
                
                # æ ¡éªŒåæ ‡æ˜¯å¦åœ¨çŸ©é˜µèŒƒå›´å†…
                if 0 <= pixel_row < self.geo_info['height'] and 0 <= pixel_col < self.geo_info['width']:
                    # å¡«å……æ•´ä¸ªåƒç´ å—ï¼ˆè€Œéå•ä¸ªåƒç´ ï¼‰ï¼Œç¡®ä¿å€¼èƒ½æ˜¾ç¤º
                    pseudo_label_matrix[pixel_row:row_end, pixel_col:col_end] = label_value
                    valid_fill += 1
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\nâœ… ä¼ªæ ‡ç­¾æå–å®Œæˆï¼š")
        print(f"   æ€»å¤„ç†èŠ‚ç‚¹æ•°ï¼š{total_nodes}")
        print(f"   éé›¶ä¼ªæ ‡ç­¾æ•°ï¼š{non_zero_label_count}")
        print(f"   æœ‰æ•ˆå¡«å……åƒç´ å—æ•°ï¼š{valid_fill}")
        print(f"   ä¼ªæ ‡ç­¾çŸ©é˜µéé›¶åƒç´ æ•°ï¼š{np.count_nonzero(pseudo_label_matrix)}")
        print(f"   ä¼ªæ ‡ç­¾çŸ©é˜µå€¼èŒƒå›´ï¼š{pseudo_label_matrix.min():.4f} ~ {pseudo_label_matrix.max():.4f}")
        
        return pseudo_label_matrix
    
    def _process_batch(self, batch):
        """å¤„ç†æ¨ç†æ‰¹æ¬¡æ•°æ®"""
        batch = batch.to(self.device)
        x = batch.x
        edge_index = batch.edge_index
        batch_idx = batch.batch
        
        if hasattr(batch, 'edge_attr') and batch.edge_attr is not None and batch.edge_attr.nelement() > 0:
            edge_attr = batch.edge_attr
        else:
            edge_attr = torch.zeros((edge_index.shape[1], self.config['edge_dim']), device=self.device)
        
        if hasattr(batch, 'lap_pe') and batch.lap_pe is not None:
            lap_pe = batch.lap_pe
        else:
            lap_pe = torch.zeros((batch.num_nodes, self.config['pe_dim']), device=self.device)
        
        return x, edge_index, edge_attr, lap_pe, batch_idx, batch.pos
    
    def infer(self):
        """æ ¸å¿ƒæ¨ç†æµç¨‹"""
        print("\n===== å¼€å§‹æ¨ç† =====")
        self.model.eval()
        infer_loader = DataLoader(
            self.infer_dataset,
            batch_size=self.config['infer_batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers']
        )
        
        all_node_preds = []
        all_positions = []
        
        with torch.no_grad():
            for batch in tqdm(infer_loader, desc="æ¨ç†è¿›åº¦"):
                x, edge_index, edge_attr, lap_pe, batch_idx, pos = self._process_batch(batch)
                
                # å‰å‘æ¨ç†
                node_pred, global_pred = self.model(
                    x=x, edge_index=edge_index, edge_attr=edge_attr, batch=batch_idx, lap_pe=lap_pe
                )
                # node_pred = torch.clamp(node_pred, min=0.0, max=14.0)  # å…³é”®ï¼é˜»æ­¢æ•°å€¼çˆ†ç‚¸
                # # è¿‡æ»¤NaN/Infï¼ˆPerformeræ˜“å‡ºç°ï¼ŒåŒé‡å…œåº•ï¼‰
                # node_pred = torch.nan_to_num(node_pred, nan=0.0, posinf=14.0, neginf=0.0)
                # æå–log(1+GDP)åˆ—
                node_log_gdp = node_pred[:, 1:2]
                
                all_node_preds.append(node_log_gdp.cpu().numpy())
                all_positions.append(pos.cpu().numpy())
        
        # åˆå¹¶ç»“æœ
        node_preds = np.vstack(all_node_preds)
        positions = np.vstack(all_positions)
        
        # è¿˜åŸåŸå§‹GDPï¼ˆæ¨¡å‹é¢„æµ‹å€¼ï¼‰
        raw_gdp = restore_gdp(node_preds.squeeze())
        
        # æ„å»ºGDPåˆ†å¸ƒçŸ©é˜µï¼ˆé¢„æµ‹å€¼ï¼‰
        gdp_matrix = np.zeros((self.geo_info['height'], self.geo_info['width']), dtype=np.float32)
        

        
        # è¡Œåˆ—åˆ†å¼€å¡«å……çŸ©é˜µ
        for i, (row, col) in enumerate(positions):
            pixel_row = int(row * self.rs_to_nl_ratio_row)
            pixel_col = int(col * self.rs_to_nl_ratio_col)
            row_end = min(pixel_row + self.rs_to_nl_ratio_row, self.geo_info['height'])
            col_end = min(pixel_col + self.rs_to_nl_ratio_col, self.geo_info['width'])
            
            if pixel_row < self.geo_info['height'] and pixel_col < self.geo_info['width']:
                gdp_matrix[pixel_row:row_end, pixel_col:col_end] = raw_gdp[i]
        
        # ç”Ÿæˆçƒ­åŠ›å›¾çŸ©é˜µï¼ˆé¢„æµ‹å€¼ï¼‰
        gdp_heatmap_matrix = normalize_to_255(gdp_matrix)
        
        # ç”Ÿæˆä¼ªæ ‡ç­¾çƒ­åŠ›å›¾çŸ©é˜µï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        pseudo_label_heatmap_matrix = normalize_to_255(self.pseudo_label_matrix)
        
        # ä¿å­˜ç»“æœï¼ˆå«é¢„æµ‹å€¼+ä¼ªæ ‡ç­¾TIFFï¼‰
        self._save_results(
            node_preds, raw_gdp, gdp_matrix, gdp_heatmap_matrix,
            self.pseudo_label_matrix, pseudo_label_heatmap_matrix
        )
        
        print("\n===== æ¨ç†å®Œæˆ =====")
        print(f"ğŸ“Š ç»“æœæ±‡æ€»ï¼š")
        print(f"   - æ¨¡å‹é¢„æµ‹GDPèŒƒå›´ï¼š{np.min(raw_gdp):.2f} ~ {np.max(raw_gdp):.2f} ä¸‡å…ƒ")
        print(f"   - ä¼ªæ ‡ç­¾GDPèŒƒå›´ï¼š{np.min(self.pseudo_label_matrix):.2f} ~ {np.max(self.pseudo_label_matrix):.2f} ä¸‡å…ƒ")
        print(f"   - çƒ­åŠ›å›¾èŒƒå›´ï¼š0.0 ~ 255.0ï¼ˆåœ°ç†ä¿¡æ¯ä¸€è‡´ï¼‰")
        print(f"   - ç»“æœä¿å­˜ç›®å½•ï¼š{self.output_dir}")
        print(f"   - è¾“å‡ºæ–‡ä»¶ï¼š")
        print(f"     âœ” {self.config['infer_county']}_raw_gdp.tifï¼ˆæ¨¡å‹é¢„æµ‹ï¼‰")
        print(f"     âœ” {self.config['infer_county']}_gdp_heatmap.tifï¼ˆé¢„æµ‹çƒ­åŠ›å›¾ï¼‰")
        print(f"     âœ” {self.config['infer_county']}_pseudo_label.tifï¼ˆä¼ªæ ‡ç­¾ï¼‰")
        print(f"     âœ” {self.config['infer_county']}_pseudo_label_heatmap.tifï¼ˆä¼ªæ ‡ç­¾çƒ­åŠ›å›¾ï¼‰")
        
        return {
            'raw_gdp_matrix': gdp_matrix,
            'raw_gdp_tif_path': os.path.join(self.output_dir, f"{self.config['infer_county']}_raw_gdp.tif"),
            'gdp_heatmap_matrix': gdp_heatmap_matrix,
            'gdp_heatmap_tif_path': os.path.join(self.output_dir, f"{self.config['infer_county']}_gdp_heatmap.tif"),
            'pseudo_label_matrix': self.pseudo_label_matrix,
            'pseudo_label_tif_path': os.path.join(self.output_dir, f"{self.config['infer_county']}_pseudo_label.tif"),
            'pseudo_label_heatmap_tif_path': os.path.join(self.output_dir, f"{self.config['infer_county']}_pseudo_label_heatmap.tif")
        }
    
    def _save_results(self, node_preds, raw_gdp, gdp_matrix, gdp_heatmap_matrix,
                     pseudo_label_matrix, pseudo_label_heatmap_matrix):
        """ä¿å­˜æ‰€æœ‰æ¨ç†ç»“æœï¼ˆæ–°å¢ä¼ªæ ‡ç­¾TIFFï¼‰"""
        # 1. ä¿å­˜çŸ©é˜µæ–‡ä»¶ï¼ˆnpyï¼‰
        np.save(os.path.join(self.output_dir, f"{self.config['infer_county']}_log_gdp_node_preds.npy"), node_preds)
        np.save(os.path.join(self.output_dir, f"{self.config['infer_county']}_raw_gdp_matrix.npy"), gdp_matrix)
        np.save(os.path.join(self.output_dir, f"{self.config['infer_county']}_gdp_heatmap_matrix.npy"), gdp_heatmap_matrix)
        np.save(os.path.join(self.output_dir, f"{self.config['infer_county']}_pseudo_label_matrix.npy"), pseudo_label_matrix)
        
        # 2. ä¿å­˜æ¨¡å‹é¢„æµ‹GDP TIFF
        write_tif(
            gdp_matrix,
            self.geo_info,
            os.path.join(self.output_dir, f"{self.config['infer_county']}_raw_gdp.tif")
        )
        
        # 3. ä¿å­˜é¢„æµ‹çƒ­åŠ›å›¾ TIFF
        write_tif(
            gdp_heatmap_matrix,
            self.geo_info,
            os.path.join(self.output_dir, f"{self.config['infer_county']}_gdp_heatmap.tif")
        )
        
        # 4. ä¿å­˜ä¼ªæ ‡ç­¾ TIFFï¼ˆæ ¸å¿ƒæ–°å¢ï¼‰
        write_tif(
            pseudo_label_matrix,
            self.geo_info,
            os.path.join(self.output_dir, f"{self.config['infer_county']}_pseudo_label.tif")
        )
        
        # 5. ä¿å­˜ä¼ªæ ‡ç­¾çƒ­åŠ›å›¾ TIFFï¼ˆç”¨äºå¯è§†åŒ–å¯¹æ¯”ï¼‰
        write_tif(
            pseudo_label_heatmap_matrix,
            self.geo_info,
            os.path.join(self.output_dir, f"{self.config['infer_county']}_pseudo_label_heatmap.tif")
        )
        
        # 6. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–°å¢ä¼ªæ ‡ç­¾ç»Ÿè®¡ï¼‰
        stats = {
            'county': self.config['infer_county'],
            'log_gdp_mean': float(np.mean(node_preds)),
            'log_gdp_std': float(np.std(node_preds)),
            # æ¨¡å‹é¢„æµ‹ç»Ÿè®¡
            'pred_raw_gdp_mean': float(np.mean(raw_gdp)),
            'pred_raw_gdp_std': float(np.std(raw_gdp)),
            'pred_raw_gdp_min': float(np.min(raw_gdp)),
            'pred_raw_gdp_max': float(np.max(raw_gdp)),
            # ä¼ªæ ‡ç­¾ç»Ÿè®¡
            'pseudo_label_mean': float(np.mean(pseudo_label_matrix[pseudo_label_matrix > 0])),
            'pseudo_label_std': float(np.std(pseudo_label_matrix[pseudo_label_matrix > 0])),
            'pseudo_label_min': float(np.min(pseudo_label_matrix)),
            'pseudo_label_max': float(np.max(pseudo_label_matrix)),
            'pseudo_label_nonzero_count': int(np.count_nonzero(pseudo_label_matrix)),
            # å…¶ä»–ä¿¡æ¯
            'rs_to_nl_ratio_row': self.rs_to_nl_ratio_row,
            'rs_to_nl_ratio_col': self.rs_to_nl_ratio_col,
            'infer_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
            'note': 'pseudo_labelæ˜¯å›¾æ„å»ºé˜¶æ®µç”Ÿæˆçš„åŸå§‹ä¼ªæ ‡ç­¾ï¼Œéæ¨¡å‹é¢„æµ‹å€¼'
        }
        
        with open(os.path.join(self.output_dir, f"{self.config['infer_county']}_stats.json"), 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=4, ensure_ascii=False)
        
        print(f"âœ… æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜ï¼ˆå«ä¼ªæ ‡ç­¾TIFFï¼‰")

# ==================== é…ç½®åŠ è½½ + ä¸»å‡½æ•° ====================
def load_infer_config(config_path):
    """åŠ è½½æ¨ç†YAMLé…ç½®"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è¡¥å……é»˜è®¤å€¼
    default_config = {
        'infer_batch_size': 8,
        'multi_gpu': False,
        'infer_output_dir': './infer_results',
        'num_workers': 0,
        'patch_size': 12,
        'lap_pe_k': 12,
        'stride': 6,
        'min_nodes_threshold': 5
    }
    for k, v in default_config.items():
        if k not in config:
            config[k] = v
    
    # æ ¡éªŒå¿…å¡«å‚æ•°ï¼ˆæ–°å¢scaler_pathï¼‰
    required = [
        'checkpoint_path', 'scaler_path', 'infer_county',
        'rs_tif_path', 'gdp_file_path', 'patch_size', 'lap_pe_k',
        'model_path', 'remote_sensing_dir', 'nl_dir', 'landuse_dir',
        'population_dir', 'output_dir', 'model_name', 'poi_dir',
        'features_dir'  # æ–°å¢ï¼šç‰¹å¾æ–‡ä»¶ç›®å½•
    ]
    for k in required:
        if k not in config:
            raise ValueError(f"é…ç½®ç¼ºå¤±å¿…å¡«é¡¹ï¼š{k}")
    
    return config

def main(args):
    """æ¨ç†ä¸»å‡½æ•°"""
    config = load_infer_config(args.config_path)
    print("="*60)
    print(f"æ¨ç†é…ç½®ï¼š")
    print(f"   - æ¨ç†å¿ï¼š{config['infer_county']}")
    print(f"   - æ¨¡å‹æƒé‡ï¼š{config['checkpoint_path']}")
    print(f"   - é¥æ„Ÿå›¾è·¯å¾„ï¼š{config['rs_tif_path']}")
    print(f"   - è¾“å‡ºç›®å½•ï¼š{config['infer_output_dir']}")
    print(f"   - è¾“å‡ºæ ¼å¼ï¼šTIFFï¼ˆæ¨¡å‹é¢„æµ‹ + ä¼ªæ ‡ç­¾ï¼‰")
    print("="*60)
    
    inferencer = GraphGDPInferencer(config)
    results = inferencer.infer()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='GraphGDPæ¨¡å‹æ¨ç†ï¼ˆæå–å›¾æ„å»ºé˜¶æ®µä¼ªæ ‡ç­¾ï¼‰')
    parser.add_argument('--config_path', type=str, default='config/GraphGDP_infer.yaml',
                        help='æ¨ç†YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    main(args)